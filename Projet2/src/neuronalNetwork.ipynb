{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Développement d'un perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint, choice\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Mise en place d'un perceptron simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la variable w contient les poids synaptiques du neurone (vecteur à 3 lignes, la première ligne correspond à au seuil)\n",
    "# la variable x contient \n",
    "def perceptron_simple(x, w, active):\n",
    "    x0 = np.concatenate(([1], x)) # x0 = 1\n",
    "    v = np.sum(np.multiply(x0, w))\n",
    "\n",
    "    if active == 0:\n",
    "        return np.sign(v)\n",
    "    elif active == 1:\n",
    "        return np.tanh(v)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "\n",
    "# Affichage des entrées sous forme de graphique\n",
    "def getClass(p):\n",
    "  if p >= 0:\n",
    "    return 'g'\n",
    "  else:\n",
    "    return 'r'\n",
    "\n",
    "# Affichage des entrées sous forme de graphique avec plusieurs classes\n",
    "def getClass(p, classes = [-1, 0, 1, 2, 3 ]):\n",
    "  if p == classes[0]:\n",
    "    return 'g'\n",
    "  elif p == classes[1]:\n",
    "    return 'b'\n",
    "  elif p == classes[2]:\n",
    "    return 'r'\n",
    "  elif p == classes[3]:\n",
    "    return 'y'\n",
    "  elif p == classes[4]:\n",
    "    return 'c'\n",
    "\n",
    "def initGrid(xmin, xmax, ymin, ymax):\n",
    "  x = np.linspace(xmin,xmax,50)\n",
    "  plt.xlim(xmin,xmax)\n",
    "  plt.ylim(ymin,ymax)\n",
    "  plt.grid()\n",
    "\n",
    "def putPoints(X, YD, classes = [-1, 0, 1, 2, 3 ]):\n",
    "  for p in range(len(YD)):\n",
    "    plt.plot(X[p][0], X[p][1], getClass(YD[p], classes) + 'o')\n",
    "\n",
    "def putLine(xmin, xmax, w):\n",
    "  x = np.linspace(xmin,xmax,50)\n",
    "  y = (w[0] + x*w[1]) / (-w[2])\n",
    "  plt.plot(x, y)\n",
    "\n",
    "def putMultipleLines(xmin, xmax, multi_w):\n",
    "  x = np.linspace(xmin,xmax,50)\n",
    "  for i in range(len(multi_w)):\n",
    "    y = (multi_w[i][0] + x*multi_w[i][1]) / (-multi_w[i][2])\n",
    "    plt.plot(x, y)\n",
    "\n",
    "def showGrid(title = ''):\n",
    "  plt.title(title)\n",
    "  plt.show()  \n",
    "\n",
    "\n",
    "# afficher la droite séparatrice associée aux poids du neurone\n",
    "def droite_separatrice(X, YD, w, xmin = -2, xmax = 2, ymin = -2, ymax = 2, titre = '', classes = [-1, 0, 1, 2, 3 ]):\n",
    "\n",
    "  initGrid(xmin, xmax, ymin, ymax)\n",
    "  putPoints(X, YD, classes)\n",
    "  putLine(xmin, xmax, w)\n",
    "  showGrid(titre)\n",
    "\n",
    "def droite_separatrice_multiple(X, YD, multi_w, xmin = -2, xmax = 2, ymin = -2, ymax = 2, titre = '', classes = [-1, 0, 1, 2, 3 ]):\n",
    "\n",
    "  initGrid(xmin, xmax, ymin, ymax)\n",
    "  putPoints(X, YD, classes)\n",
    "  putMultipleLines(xmin, xmax, multi_w)\n",
    "  showGrid(titre)\n",
    "\n",
    "def print_in_red(text):\n",
    "  print(\"\\033[91m {}\\033[00m\" .format(text))\n",
    "\n",
    "def print_in_green(text):\n",
    "  print(\"\\033[92m {}\\033[00m\" .format(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test du perceptron avec l'exemple du OU logique vu en cours (phi(x) = sign(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test perceptron simple sur la porte OU\n",
    "\n",
    "# poids\n",
    "w = np.array([-0.5, 1, 1])\n",
    "\n",
    "# entrées\n",
    "x = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "\n",
    "# sortie\n",
    "res = []\n",
    "for i in range(4) :\n",
    "  res.append(perceptron_simple(x[i], w, 0))\n",
    "\n",
    "droite_separatrice(x, res, w, -0.5, 1.5, -0.5, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Etude de l'apprentissage\n",
    "#### 1.2.1 Programmation apprentissage Widrow-hoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x contient l'ensemble d'apprentissage (matrice à 2 lignes et n colonnes).\n",
    "# yd[i] indique la réponse désirée pour chaque élément x[:,i]. yd est un vecteur à 1 ligne et n colonnes de valeurs -1 ou +1 (classification à 2 classes).\n",
    "# epoch le nomble d'itérations sur l'ensemble d'apprentissage.\n",
    "# batch_size le nombre d'individus de l'ensemble d'apprentissage traités avant mise à jour des poids \n",
    "def apprentissage_widrow(x, yd, epoch, batch_size) :\n",
    "    w = np.random.rand(len(x[0]) + 1) * 2 - 1 # w contient les poids synaptiques du neurone après apprentissage (vecteur à 3 lignes, dont la première ligne correspond à au seuil)\n",
    "    erreur = np.zeros((epoch)) # l'erreur cumulée calculée pour passage complet sur l'ensemble d'apprentissage (somme (yd(i) - y(i))²). La variable erreur est donc un vecteur de taille égale au nombre d'itération.\n",
    "    alpha = 0.1 # le coefficient d'apprentissage\n",
    "\n",
    "    droite_separatrice(x, yd, w, -5, 5, -5, 5, 'Droite séparatrice initiale')\n",
    "\n",
    "    for i in range(epoch):\n",
    "        tmp_w = w\n",
    "        erreur[i] = 0\n",
    "        for j in range(len(x)):\n",
    "            y = perceptron_simple(x[j], w, 1)\n",
    "            erreur[i] += (yd[j] - y) ** 2\n",
    "            tmp_w += alpha * (yd[j] - y) * np.concatenate(([1], x[j]))\n",
    "\n",
    "            # mise à jour des poids à chaque batch_size\n",
    "            if (j % batch_size) == 0:\n",
    "                w = tmp_w\n",
    "        \n",
    "        # affichage de la droite séparatrice associée aux poids du neurone\n",
    "        titre = 'Itération ' + str(i+1) + '\\nerreur cumulée = ' + str(erreur[i])\n",
    "        droite_separatrice(x, yd, w, -5, 5, -5, 5, titre)\n",
    "\n",
    "        if erreur[i] == 0:\n",
    "            break\n",
    "\n",
    "    return w, erreur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pour le OU\n",
    "x = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "yd = np.array([-1, 1, 1, 1])\n",
    "\n",
    "w, erreur = apprentissage_widrow(x, yd, 20, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Test 1 simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data p2_d1.txt\n",
    "data = np.loadtxt('../data/p2_d1.txt')\n",
    "x = list(map(list,zip(data[0],data[1])))\n",
    "yd = [-1]*25 + [1]*25\n",
    "\n",
    "# application de l'algorithme de Widrow-Hoff\n",
    "w, erreur = apprentissage_widrow(x, yd, 10, 5)\n",
    "print(\"w : \", w, \"\\nerreur : \", erreur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data p2_d2.txt\n",
    "data = np.loadtxt('../data/p2_d2.txt')\n",
    "x = list(map(list,zip(data[0],data[1])))\n",
    "yd = [-1]*25 + [1]*25\n",
    "\n",
    "# application de l'algorithme de Widrow-Hoff\n",
    "w, erreur = apprentissage_widrow(x, yd, 15, 5)\n",
    "print(\"w : \", w, \"\\nerreur : \", erreur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Perceptron multicouches\n",
    "### 1.3.1 Mise en place d’un perceptron multicouche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def perceptron(x, w, activation_function): # comme perceptron_simple mais avec une fonction d'activation en paramètre\n",
    "    return activation_function(np.dot(w, np.concatenate(([1], x))))\n",
    "\n",
    "def multiperceptron(x,w1,w2):\n",
    "    y1 = perceptron(x, w1[:, 0], sigmoid)\n",
    "    y2 = perceptron(x, w1[:, 1], sigmoid)\n",
    "\n",
    "    res_couche_cachee = np.array([y1, y2])\n",
    "    y = perceptron(res_couche_cachee, w2, sigmoid)\n",
    "    return (y1, y2, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparer le résultat ”informatique” avec la sortie attendue calculée sur ”le papier”**\n",
    "\n",
    "Sur papier:\n",
    "\n",
    "1er  neurone: (1 * (-0.5))  + (1 * 2) + (1 * (-1)) = 0.5 => sigmoid(0.5) = 0.6224593312018546\n",
    "2ème neurone: (1 * 0.5) + (1 * 0.5) + (1 * 1) = 2 => sigmoid(2) = 0.8807970779778823\n",
    "\n",
    "3ème neurone: (1 * 2) + (-1 * 0.6224593312018546) + (1 * 0.8807970779778823) = 2.258338 => sigmoid(2.258338) = 0.9053673095402572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test avec l'exepmle du cours\n",
    "x  = np.array([1, 1])\n",
    "w1 = np.array([[-0.5, 0.5], [2.0, 0.5], [-1.0, 1.0]])\n",
    "w2 = np.array([2.0, -1.0, 1.0])\n",
    "\n",
    "print(multiperceptron(x, w1, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Programmation apprentissage multicouches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def multiperceptron_widrow(x,yd,Epoch,Batch_size):\n",
    "\n",
    "    w1 = np.random.rand(len(x[0]) + 1, 2) * 2 - 1\n",
    "    w2 = np.random.rand(3) * 2 - 1\n",
    "    erreur = np.zeros((Epoch))\n",
    "    alpha = 0.5\n",
    "\n",
    "    # affichage de la droite séparatrice associée aux poids du neurone\n",
    "    titre = 'droite séparatrice initiale'\n",
    "    # créer le tableau des droites séparatrices à afficher avec les poids\n",
    "    multi_w = np.array([w1[:, 0], w1[:, 1], w2])\n",
    "    # affichage de toutes les droites séparatrices avec droite_separatrice_multiple\n",
    "    droite_separatrice_multiple(x, yd, multi_w, -5, 5, -5, 5, titre)\n",
    "\n",
    "    for i in range(Epoch):\n",
    "        tmp_w1 = w1\n",
    "        tmp_w2 = w2\n",
    "        erreur[i] = 0\n",
    "        for j in range(len(x)):\n",
    "            individu = x[j]\n",
    "            (y1, y2, y) = multiperceptron(individu, w1, w2)\n",
    "\n",
    "            erreur[i] += (yd[j] - y) ** 2\n",
    "\n",
    "            rf = (yd[j] - y) * derive(y)\n",
    "            r11 = rf * w2[1] * derive(y1)\n",
    "            r12 = rf * w2[2] * derive(y2)\n",
    "\n",
    "            tmp_w1[:, 0] += alpha * r11 * np.concatenate(([1], individu))\n",
    "            tmp_w1[:, 1] += alpha * r12 * np.concatenate(([1], individu))\n",
    "            tmp_w2 += alpha * rf * np.array([1, y1, y2])\n",
    "\n",
    "            if (j % Batch_size) == 0:\n",
    "                w1 = tmp_w1\n",
    "                w2 = tmp_w2\n",
    "        \n",
    "        # evolution de l'erreur\n",
    "\n",
    "\n",
    "        if erreur[i] < 0.001:\n",
    "            break\n",
    "\n",
    "    titre = 'Itération ' + str(i+1) + '\\nerreur cumulée = ' + str(erreur[i])\n",
    "    multi_w = np.array([w1[:, 0], w1[:, 1], w2])\n",
    "    droite_separatrice_multiple(x, yd, multi_w, -5, 5, -5, 5, titre)\n",
    "\n",
    "    return w1, w2, erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test avec le XOR  \n",
    "x = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "yd = np.array([0, 1, 1, 0])\n",
    "\n",
    "w1, w2, erreur = multiperceptron_widrow(x, yd, 10000, 2)\n",
    "print(\"w1 : \", w1, \"\\nw2 : \", w2, \"\\nerreur : \", erreur)\n",
    "\n",
    "# graphique de l'erreur en rouge\n",
    "plt.plot(erreur, 'r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep et Full-connected : discrimination d'une image\n",
    "### Approche basée Descripteurs (basée modèle)\n",
    "#### Calcul des descripteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import cv2\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom, RandomContrast, RandomTranslation, Rescaling\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = [\"Jungle\", \"Plage\", \"Monuments\", \"Bus\", \"Dinosaures\", \"Eléphants\",\"Fleurs\", \"Chevaux\", \"Montagne\" , \"Plats\"]\n",
    "descriptors = [\"WangSignaturesJCD\", \"WangSignaturesPHOG\", \"WangSignaturesCEDD\", \"WangSignaturesFCTH\", \"WangSignaturesFuzzyColorHistogr\"]\n",
    "\n",
    "img_list = pd.read_excel('../data/WangSignatures.xls', descriptors[0], index_col = 0,header=None).index\n",
    "\n",
    "label=np.zeros(1000,'int')\n",
    "Target = np.zeros((1000,10),'int')\n",
    "\n",
    "index = 0\n",
    "for img_name in img_list:\n",
    "    lbl = int(int(img_name[0:-4])/100) # remove '.jpg' (4 last chars) and divide by 100\n",
    "    label[index]= lbl\n",
    "    Target[index][lbl]=1\n",
    "    index += 1\n",
    "\n",
    "# get the Mesures and splited data with a descriptors name\n",
    "def split_with_descriptor(descriptor):\n",
    "    X = (pd.read_excel('../data/WangSignatures.xls', \"WangSignatures\" + descriptor, index_col = 0,header=None))\n",
    "    Mesures = X.values\n",
    "\n",
    "    # split the data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(Mesures, Target, test_size=0.2, random_state=1, stratify=label)\n",
    "\n",
    "    # return the name of the descriptor the Mesures and the splited data\n",
    "    return descriptor, Mesures, X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHOG = split_with_descriptor(\"PHOG\")\n",
    "JCD = split_with_descriptor(\"JCD\")\n",
    "CEDD = split_with_descriptor(\"CEDD\")\n",
    "FCTH = split_with_descriptor(\"FCTH\")\n",
    "FuzzyColorHistogr = split_with_descriptor(\"FuzzyColorHistogr\")\n",
    "\n",
    "# split with all the descriptors\n",
    "def split_with_all_descriptors():\n",
    "    Mesures = np.concatenate((PHOG[1], JCD[1], CEDD[1], FCTH[1], FuzzyColorHistogr[1]), axis=1)\n",
    "\n",
    "    # split the data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(Mesures, Target, test_size=0.2, random_state=1, stratify=label)\n",
    "\n",
    "    # return the name of the descriptor the Mesures and the splited data\n",
    "    return \"All descriptors\", Mesures, X_train, X_test, Y_train, Y_test\n",
    "\n",
    "All_descriptors = split_with_all_descriptors()\n",
    "\n",
    "# an array with all the descriptors\n",
    "DESCRIPTORS_ARRAY = [PHOG, JCD, CEDD, FCTH, FuzzyColorHistogr, All_descriptors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util (show accuracy and confusion matrix, and loss)\n",
    "def show_metrics(Y_test, Y_pred, history, name = \"\"):\n",
    "    # show accuracy\n",
    "    plt.title('Accuracy ' + name)\n",
    "    plt.plot(history.history['accuracy'], 'g--' )\n",
    "    plt.plot(history.history['val_accuracy'], 'g')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.show()\n",
    "\n",
    "    # show confusion matrix\n",
    "    confusionMatrix = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix, display_labels=classes)\n",
    "    disp = disp.plot(include_values=True, cmap=plt.cm.Blues, ax=None, xticks_rotation='vertical')\n",
    "    plt.title('Confusion matrix ' + name)\n",
    "    plt.show()\n",
    "\n",
    "    # show loss\n",
    "    plt.title('Loss ' + name)\n",
    "    plt.plot(history.history['loss'], 'r--' )\n",
    "    plt.plot(history.history['val_loss'], 'r')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise en place d’un système de discrimination basée structure Full-Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with each descriptor (and all the descriptors)\n",
    "for DESCRIPTOR in DESCRIPTORS_ARRAY:\n",
    "\n",
    "    name, X_train, X_test, Y_train, Y_test = DESCRIPTOR[0], DESCRIPTOR[2], DESCRIPTOR[3], DESCRIPTOR[4], DESCRIPTOR[5]\n",
    "    print(\"Training with \" + name)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, epochs=30, batch_size=32,validation_split=0.2, verbose=0)\n",
    "    model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "    Y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "    show_metrics(Y_test, Y_pred, history, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec plusieurs test, le meuilleur discriminateur est le CEDD (avec 0.9 d'accuracy) \n",
    "# donc on va le garder pour la suite\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = CEDD[2], CEDD[3], CEDD[4], CEDD[5]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=30, batch_size=32,validation_split=0.2, verbose=0)\n",
    "model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "Y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "# make a test with random images\n",
    "for i in range(30):\n",
    "    # get a random number from the test set\n",
    "    rand = np.random.randint(0, len(X_test))\n",
    "    # predict the class\n",
    "    pred = model.predict(np.array([X_test[rand]]), verbose=0)\n",
    "    # get the percentage of the highest prediction\n",
    "    percentage = np.max(pred)\n",
    "    # get the class with the highest probability\n",
    "    pred = np.argmax(pred)\n",
    "    # get the real class\n",
    "    real = np.argmax(Y_test[rand])\n",
    "    # print the result\n",
    "    # if the prediction is correct, print it in green\n",
    "    if pred == real:\n",
    "        print_in_green(\"Predicted: \" + classes[pred] + \", Real: \" + classes[real] + \" (probability: \" + str(percentage) + \"%)\")\n",
    "    # if the prediction is wrong, print it in red\n",
    "    else:\n",
    "        print_in_red(\"Predicted: \" + classes[pred] + \", Real: \" + classes[real] + \" (probability: \" + str(percentage) + \"%)\")\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approche \"Deep\" (basée Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Target = np.zeros([1000, 10], 'int')\n",
    "index = 0\n",
    "\n",
    "for img_name in img_list:\n",
    "    img = plt.imread(\"../data/Wang/\" + img_name)\n",
    "    img = img[0:256, 0:256, :]\n",
    "    X.append(img)\n",
    "\n",
    "    lbl = int(img_name[0:-4]) // 100\n",
    "    Target[index, lbl] = 1\n",
    "    index += 1\n",
    "\n",
    "inputShape = X[0].shape\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test show one image\n",
    "# peek a random image between 0 and 999\n",
    "i = np.random.randint(0, 1000)\n",
    "plt.title('Image : ' + img_list[i] + '\\nClass : ' + classes[np.argmax(Target[i])])\n",
    "plt.imshow(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Target, test_size=0.2, random_state=1)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=inputShape),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs=15, batch_size=32,validation_split=0.2, verbose=0)\n",
    "model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "Y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "show_metrics(Y_test, Y_pred, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(8, (5,5), activation='relu', padding='same', input_shape=inputShape),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(16, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(32, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(256, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(512, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs=15, batch_size=32,validation_split=0.2, verbose=0)\n",
    "model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "Y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "show_metrics(Y_test, Y_pred, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_transfer_learning = Xception(weights='imagenet', include_top=False, input_shape=inputShape)\n",
    "\n",
    "for layer in modele_transfer_learning.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = modele_transfer_learning.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "out = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=modele_transfer_learning.input, outputs=out)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs=15, batch_size=32,validation_split=0.2, verbose=0)\n",
    "model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "Y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "show_metrics(Y_test, Y_pred, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "dataAugmentation = Sequential([\n",
    "    RandomFlip('horizontal'),\n",
    "    RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "dataSet = Dataset.from_tensor_slices((X_train, Y_train))\n",
    "dataSet.map(lambda x, y: (dataAugmentation(x, training=True), y))\n",
    "\n",
    "# model with data augmentation\n",
    "model = Sequential([\n",
    "    dataAugmentation,\n",
    "    Conv2D(8, (5,5), activation='relu', padding='same', input_shape=inputShape),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(16, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(32, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(256, (5,5), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(512, (5,5), activation='relu', padding='same'),\n",
    "\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs=15, batch_size=32,validation_split=0.2, verbose=0)\n",
    "model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "Y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "show_metrics(Y_test, Y_pred, history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "data_augmentation = Sequential()\n",
    "\n",
    "data_augmentation.add(RandomFlip(\"horizontal\"))\n",
    "data_augmentation.add(RandomRotation(0.1))\n",
    "\n",
    "dataset = Dataset.from_tensor_slices((X_train, Y_train))\n",
    "dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(data_augmentation)\n",
    "model.add(Conv2D(32, (5, 5), activation='relu',padding='same', input_shape=inputShape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding='same', input_shape=inputShape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, epochs=30,batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "Y_predict = model.predict(X_test, verbose=0)\n",
    "\n",
    "show_metrics(Y_test, Y_predict, history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4428cbe1ba9314b3551257500664b995dcc328d303584ff4cad6f1a703111ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
