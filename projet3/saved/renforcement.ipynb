{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5HGSGxhnEfS"
      },
      "source": [
        "# 1. Développement d'un jeu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "16u0v0mvnEfU"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6bQNPZzonEfV"
      },
      "outputs": [],
      "source": [
        "# configuration\n",
        "GAMMA = 0.96\n",
        "ALPHA = 0.81\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dWZoG4tanEfW"
      },
      "outputs": [],
      "source": [
        "#for the space\n",
        "\"\"\"\n",
        "by default the default space is used\n",
        "the space is a 2D array 4 * 4 of characters\n",
        "the characters are: \n",
        "    'S' : the starting point\n",
        "    '_' : empty space\n",
        "    'J' : the goal\n",
        "    'D' : a dragon\n",
        "the default space is:\n",
        "    S___\n",
        "    D_D_ \n",
        "    ___D\n",
        "    _D_J\n",
        "\"\"\"\n",
        "\n",
        "# the default space\n",
        "default_space = [\n",
        "        ['S', '_', '_', '_'],\n",
        "        ['D', '_', 'D', '_'],\n",
        "        ['_', '_', '_', 'D'],\n",
        "        ['_', 'D', '_', 'J']\n",
        "    ]\n",
        "\n",
        "def get_default_space():\n",
        "    return default_space\n",
        "\n",
        "# a random space with number of lines and columns and a number of dragons\n",
        "def get_random_space(lines, columns, dragons):\n",
        "    space = []\n",
        "    for l in range(lines):\n",
        "        space.append([])\n",
        "        for c in range(columns):\n",
        "            space[l].append('_')\n",
        "\n",
        "    space[0][0] = 'S'\n",
        "    space[lines-1][columns-1] = 'J'\n",
        "    \n",
        "    i = 0\n",
        "    while i < dragons:\n",
        "        l = random.randint(0, lines-1)\n",
        "        c = random.randint(0, columns-1)\n",
        "        if space[l][c] == '_':\n",
        "            space[l][c] = 'D'\n",
        "            i += 1\n",
        "        else:\n",
        "            i -= 1\n",
        "\n",
        "    return space\n",
        "\n",
        "# pretty print the space\n",
        "def print_space(space):\n",
        "    for l in space:\n",
        "        for c in l:\n",
        "            print(c, end='| ')\n",
        "        \n",
        "        print()\n",
        "\n",
        "# get the size of the lines\n",
        "def get_lines_size(space):\n",
        "    return len(space)\n",
        "\n",
        "# get the size of the columns\n",
        "def get_columns_size(space):\n",
        "    return len(space[0])\n",
        "\n",
        "# get the size of the space\n",
        "def get_size(space):\n",
        "    return get_lines_size(space) * get_columns_size(space)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-eVBWhkOnEfX"
      },
      "outputs": [],
      "source": [
        "# a static class for the rewards\n",
        "class Rewards:\n",
        "    # the rewards for each character\n",
        "    rewards = {\n",
        "        'S': 0,\n",
        "        '_': 0,\n",
        "        'J': 1,\n",
        "        'D': -1\n",
        "    }\n",
        "\n",
        "    # a method to get the reward of a character\n",
        "    def get_reward(character):\n",
        "        return Rewards.rewards.get(character)\n",
        "\n",
        "    # a method to set the reward \n",
        "    def set_rewards(rewards):\n",
        "        Rewards.rewards = rewards\n",
        "\n",
        "    # a method to set the reward of a character\n",
        "    def set_reward(character, reward):\n",
        "        Rewards.rewards[character] = reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "amIQUyAKnEfX"
      },
      "outputs": [],
      "source": [
        "# a static class for the Directions\n",
        "class Directions:\n",
        "    # the directions\n",
        "    directions = [\"HAUT\", \"DROITE\", \"BAS\", \"GAUCHE\"]\n",
        "\n",
        "    # a method to get the size of the directions\n",
        "    def get_size():\n",
        "        return len(Directions.directions)\n",
        "\n",
        "    # a method to get the index of a direction\n",
        "    def get_index(direction):\n",
        "        return Directions.directions.index(direction)\n",
        "\n",
        "    # a method to get a random direction\n",
        "    def get_random_direction():\n",
        "        return random.choice(Directions.directions)\n",
        "\n",
        "    # a method to get the direction that maximizes the Q value\n",
        "    def get_max_direction(mat_q, state):\n",
        "        return Directions.directions[np.argmax(mat_q[state])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Tpo9NTAYnEfY"
      },
      "outputs": [],
      "source": [
        "# for the Q matrix\n",
        "# init the Q matrix with zeros and the size of the space and the directions length\n",
        "def init_mat_q(space):\n",
        "    return np.zeros((get_lines_size(space), get_columns_size(space), Directions.get_size()))\n",
        "\n",
        "# get the Q value of a state and a direction\n",
        "def get_q_value(mat_q, state, direction):\n",
        "    return mat_q[state][Directions.get_index(direction)]\n",
        "\n",
        "# update the Q matrix\n",
        "# according to state, action, reward, next_state, ALPHA and GAMMA\n",
        "def update_mat_q(mat_q, state, action, reward, next_state):\n",
        "    mat_q[state][Directions.get_index(action)] += ALPHA * (reward + GAMMA * np.max(mat_q[next_state]) - mat_q[state][Directions.get_index(action)])\n",
        "    return mat_q\n",
        "\n",
        "# pretty print the space\n",
        "def print_mat_q(mat_q, space):\n",
        "    def get_best_direction(l, c):\n",
        "        return Directions.directions[np.argmax(mat_q[l][c])]\n",
        "\n",
        "    for l in range(get_lines_size(space)):\n",
        "        for c in range(get_columns_size(space)):\n",
        "            case      = space[l][c]\n",
        "            direction = get_best_direction(l, c)\n",
        "            q_value   = str(round(get_q_value(mat_q, (l, c), direction), 2)).ljust(6)\n",
        "\n",
        "            content = case + \" (\" + q_value + \") \" + direction\n",
        "            print(content.ljust(20), end='| ')\n",
        "        print(\"\\n_______________________________________________________________________________________\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "emfLzRnwnEfY"
      },
      "outputs": [],
      "source": [
        "def isFin (space, position):\n",
        "    (l,c) = position\n",
        "\n",
        "    if(space[l][c]== 'J' or space[l][c]== 'D'):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# a method to apply an action to the player\n",
        "# returns [position, reward, fin]\n",
        "def applicaion_action(action, position, space):\n",
        "\n",
        "    # reward -1 every time\n",
        "    reward = -1\n",
        "\n",
        "    (l, c) = position\n",
        "    nextPos = position\n",
        "\n",
        "    if action == \"HAUT\":\n",
        "        nextPos = (l-1,c)\n",
        "    elif action == \"DROITE\":\n",
        "        nextPos = (l,c+1)\n",
        "    elif action == \"BAS\":\n",
        "        nextPos = (l+1,c);\n",
        "    elif action == \"GAUCHE\":\n",
        "        nextPos = (l,c-1);\n",
        "\n",
        "    # check if the next position is in the space\n",
        "    if (nextPos[0] < len(space) and nextPos[1] < len(space) and nextPos[0] >=0 and nextPos[1] >=0 ):\n",
        "        position = nextPos\n",
        "\n",
        "        # back to the starting point if a dragon is encountered\n",
        "        # get the current case in the space\n",
        "        case = space[position[0]][position[1]]\n",
        "        if case == 'D':\n",
        "            position = (0, 0)\n",
        "    \n",
        "        # set the reward\n",
        "        reward += Rewards.get_reward(case)\n",
        "\n",
        "    # check if the player is at the goal\n",
        "    fin = isFin(space, position)\n",
        "\n",
        "    # if the player is at the goal, back to the starting point\n",
        "    if fin:\n",
        "        position = (0, 0)\n",
        "\n",
        "    return [position, reward, fin]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDxUSIRynEfZ"
      },
      "source": [
        "# 2. Développement du Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TIz5A3zznEfZ"
      },
      "outputs": [],
      "source": [
        "# a class for the game\n",
        "class Game:\n",
        "    # constructor that takes :\n",
        "    # number of episodes : 10000 by default\n",
        "    # number of steps : 100 by default\n",
        "    # is_random_space : False by default\n",
        "    # a Q matrix : initialized with zeros (with the size of the space and the number of directions)\n",
        "    def __init__(self, episodes = 10000, steps = 100, is_random_space = False, mat_q = None):\n",
        "        self.episodes = episodes\n",
        "        self.steps = steps\n",
        "        # the space\n",
        "        if is_random_space:\n",
        "            self.space = get_random_space(4, 4, 3)\n",
        "        else:\n",
        "            self.space = get_default_space()\n",
        "        \n",
        "        # the Q matrix \n",
        "        if mat_q is None:\n",
        "            self.mat_q = init_mat_q(self.space)\n",
        "\n",
        "    # a method to choose an action with the epsilon greedy policy\n",
        "    def choose_action(self, state, epsilon, mat_q):\n",
        "        if random.random() < epsilon:\n",
        "            return Directions.get_random_direction()\n",
        "        else:\n",
        "            return Directions.get_max_direction(mat_q, state)\n",
        "\n",
        "    # a method to play one step (with mat_q, state, epsilon)\n",
        "    def oneStep(self, mat_q, state, epsilon, verbose):\n",
        "        # choose an action\n",
        "        action = self.choose_action(state, epsilon, mat_q)\n",
        "        if verbose:\n",
        "            print(action, end=', ')\n",
        "        # apply the action\n",
        "        new_state, reward, fin = applicaion_action(action, state, self.space)\n",
        "        # update the Q matrix\n",
        "        new_q = update_mat_q(mat_q, state, action, reward, new_state)\n",
        "        return new_q, new_state, fin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eWZfAABqnEfa",
        "outputId": "08bff82a-c1ff-4df7-d6a7-0bcf4bc2423e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total steps :  240086\n",
            "average steps :  24.0086\n",
            "[[[-21.396843   -21.24671146 -22.396843   -21.396843  ]\n",
            "  [-21.24671146 -21.396843   -21.09032444 -21.396843  ]\n",
            "  [-21.396843   -21.54096928 -22.396843   -21.24671146]\n",
            "  [-21.54096928 -21.54096928 -21.67933051 -21.396843  ]]\n",
            "\n",
            " [[  0.           0.           0.           0.        ]\n",
            "  [-21.24671146 -22.396843   -20.92742129 -22.396843  ]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [-21.54096928 -21.67933051 -22.396843   -22.396843  ]]\n",
            "\n",
            " [[-22.396843   -20.92742129 -21.24671146 -21.09032444]\n",
            "  [-21.09032444 -20.75773051 -22.396843   -21.09032444]\n",
            "  [-22.396843   -22.396843   -20.58096928 -20.92742129]\n",
            "  [  0.           0.           0.           0.        ]]\n",
            "\n",
            " [[-21.09032444 -22.396843   -21.24671146 -21.24671146]\n",
            "  [  0.           0.           0.           0.        ]\n",
            "  [-20.75773051 -20.396843   -20.58096928 -22.396843  ]\n",
            "  [  0.           0.           0.           0.        ]]]\n"
          ]
        }
      ],
      "source": [
        "# PLAY\n",
        "game = Game()\n",
        "\n",
        "total_steps = 0\n",
        "\n",
        "# apply the algorithm \n",
        "for episode in range(game.episodes):\n",
        "    # reset the position\n",
        "    position = (0, 0)\n",
        "    # calculate the epsilon\n",
        "    epsilon = game.episodes / (game.episodes + episode)\n",
        "    #print(\"epsilon : \", epsilon)\n",
        "\n",
        "    # play the game\n",
        "    for step in range(1, game.steps):\n",
        "        # play one step\n",
        "        game.mat_q, position, fin = game.oneStep(game.mat_q, position, epsilon, False)\n",
        "        #print(\"position : \", game.position)\n",
        "\n",
        "        # if the game is finished\n",
        "        if fin:\n",
        "            total_steps += step\n",
        "            break\n",
        "\n",
        "print(\"total steps : \", total_steps)\n",
        "print(\"average steps : \", total_steps / game.episodes)\n",
        "\n",
        "print(game.mat_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t6AXQlAnnEfa",
        "outputId": "cb9f7fb6-15d2-465d-eca3-52b114e72f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DROITE, BAS, BAS, DROITE, BAS, DROITE, fin de partie en 6 coups\n"
          ]
        }
      ],
      "source": [
        "# play a with the optimal policy\n",
        "for step in range(1, game.steps):\n",
        "    # play one step\n",
        "    game.mat_q, position, fin = game.oneStep(game.mat_q, position, 0, True)\n",
        "    if fin:\n",
        "        print(\"fin de partie en\", step, \"coups\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4rYr5rkcnEfa",
        "outputId": "473c5cd5-8e4b-468c-99b4-c72b82ea0859",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S (-21.25) DROITE   | _ (-21.09) BAS      | _ (-21.25) GAUCHE   | _ (-21.4 ) GAUCHE   | \n",
            "_______________________________________________________________________________________\n",
            "D (0.0   ) HAUT     | _ (-20.93) BAS      | D (0.0   ) HAUT     | _ (-21.54) HAUT     | \n",
            "_______________________________________________________________________________________\n",
            "_ (-20.93) DROITE   | _ (-20.76) DROITE   | _ (-20.58) BAS      | D (0.0   ) HAUT     | \n",
            "_______________________________________________________________________________________\n",
            "_ (-21.09) HAUT     | D (0.0   ) HAUT     | _ (-20.4 ) DROITE   | J (0.0   ) HAUT     | \n",
            "_______________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print_mat_q(game.mat_q, game.space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_XvbUyvnEfa"
      },
      "source": [
        "# Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SQdRhWhsnEfb"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import tensorflow as tf\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "u0pmQ8Zjt3Q4",
        "outputId": "d113c12c-f75f-4deb-f2b5-d73777ae8e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcaCB4TPnEfb"
      },
      "source": [
        "##### Test avec une structure 2 couches denses ayant 16 entrées (nombre de cases) et 4 sorties (4 actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rXv4ZhrfnEfb"
      },
      "outputs": [],
      "source": [
        "# a class for the game\n",
        "class DeepGame:\n",
        "    # constructor that takes :\n",
        "    # number of episodes : 10000 by default\n",
        "    # number of steps : 100 by default\n",
        "    # is_random_space : False by default\n",
        "    # vec_etat : vector of states (for deep Q learning)\n",
        "    # a model : CNN\n",
        "    def __init__(self, episodes = 10000, steps = 100, is_random_space = False, vec_etat = None, model = None):\n",
        "        self.episodes = episodes\n",
        "        self.steps = steps\n",
        "        # the space\n",
        "        if is_random_space:\n",
        "            self.space = get_random_space(4, 4, 3)\n",
        "        else:\n",
        "            self.space = get_default_space()\n",
        "\n",
        "        # the vector of states\n",
        "        if vec_etat is None:\n",
        "            self.set_default_vec_etat()\n",
        "\n",
        "        # the model\n",
        "        if model is None:\n",
        "            self.set_default_model()\n",
        "\n",
        "\n",
        "    # a method to set the vector of states\n",
        "    def set_vec_etat(self, vec_etat):\n",
        "        self.vec_etat = vec_etat\n",
        "\n",
        "    # a method to set the default vector of states\n",
        "    def set_default_vec_etat(self):\n",
        "        self.vec_etat = np.zeros((1, 16))\n",
        "        self.vec_etat[0][0] = 1\n",
        "\n",
        "    # a method to reset the vector of states\n",
        "    def reset_vec_etat(self):\n",
        "        self.vec_etat = np.zeros((1, 16))\n",
        "\n",
        "    # a method to update the vector of states\n",
        "    def update_vec_etat(self, state):\n",
        "        self.reset_vec_etat()\n",
        "        self.vec_etat[0, int(get_lines_size(self.space) * state[0] + state[1])] = 1\n",
        "\n",
        "    # a method to set the model\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    # a method to set the default model\n",
        "    def set_default_model(self):\n",
        "        self.model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(4, activation='relu', input_shape=[16]),\n",
        "            tf.keras.layers.Dense(4, activation='relu'),\n",
        "            tf.keras.layers.Dense(4),\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # a method to choose an action with the epsilon greedy policy\n",
        "    def choose_action(self, state, epsilon):\n",
        "        if random.random() < epsilon:\n",
        "            action = Directions.get_random_direction()\n",
        "            #print(\"random :\", action)\n",
        "            return action\n",
        "        else:\n",
        "            Sortie_Q = self.model(self.vec_etat)  # En entrée le vecteur symbolisant l'état\n",
        "            action = Directions.directions[np.argmax(Sortie_Q)] #On sélectionne l'action associée avec la sortie max\n",
        "            #print(\"max    :\", action)\n",
        "            return action\n",
        "     \n",
        "    # Train the model\n",
        "    def train(self, optimizer, loss_fn, verbose, saved_model_path = \"deep_Q_learning_model.h5\"):\n",
        "        # create a stable model\n",
        "        model_stable = tf.keras.models.clone_model(self.model)\n",
        "        model_stable.set_weights(self.model.get_weights())\n",
        "\n",
        "        history = np.zeros(self.episodes)\n",
        "\n",
        "        for episode in range(self.episodes):\n",
        "            # reset the position\n",
        "            position = (0, 0)\n",
        "            # calculate the epsilon\n",
        "            epsilon = self.episodes / (self.episodes + episode)\n",
        "\n",
        "            for step in range(1, self.steps):\n",
        "                sys.stdout.write(\"\\r\" + \"epoch : \" + str(episode) + \"/\" + str(self.episodes) + \", step : \" + str(step) + \"/\" + str(self.steps))\n",
        "                # play one step\n",
        "                # choose an action\n",
        "                action = self.choose_action(position, epsilon)\n",
        "                # apply the action\n",
        "                new_position, reward, fin = applicaion_action(action, position, self.space)\n",
        "                if verbose:\n",
        "                    print(action, end=\", \")\n",
        "\n",
        "                if fin:\n",
        "                    if verbose:\n",
        "                        print(\"fin de partie en\", step, \"coups\")\n",
        "                    break\n",
        "\n",
        "                # set weights of the stable model\n",
        "                if step % 10 == 0:\n",
        "                    model_stable.set_weights(self.model.get_weights())\n",
        "\n",
        "                vec_etat_next = np.zeros((1, 16)) # ca sera l'entree du reseau\n",
        "                vec_etat_next[0, int(get_lines_size(self.space) * new_position[0] + new_position[1])] = 1\n",
        "\n",
        "                # model stable predict\n",
        "                next_Q = model_stable.predict(vec_etat_next, verbose=0)\n",
        "                next_Q_max = np.max(next_Q)\n",
        "\n",
        "                # target\n",
        "                #print(\"reward : \", reward, \"next_Q_max : \", next_Q_max, \"(1-fin) : \", (1-fin))\n",
        "                target = reward + GAMMA * next_Q_max * (1 - fin)\n",
        "\n",
        "                # gradient descent\n",
        "                with tf.GradientTape() as tape:\n",
        "                    predict = self.model(self.vec_etat) # ce que l'on pense obtenir\n",
        "                    # get index of the action\n",
        "                    action_index = Directions.get_index(action)\n",
        "                    mask = tf.one_hot(action_index, Directions.get_size())\n",
        "                    val_predict = tf.reduce_sum(predict * mask, axis=1)\n",
        "                    loss = loss_fn(target, val_predict)\n",
        "\n",
        "                gradients = tape.gradient(loss, self.model.trainable_variables) #calcul du gradient de la focntion loss en fonction des variables du modèle \n",
        "                optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) # optimisation des paramètres du modèle\n",
        "                history[episode] = loss.numpy() # on récupère la valeur pour afficher l'évolution de l'erreur\n",
        "\n",
        "                # update the position\n",
        "                position = new_position\n",
        "\n",
        "        plt.plot(history, color=\"red\")\n",
        "        plt.title(\"Evolution de l'erreur\")\n",
        "        plt.show()\n",
        "\n",
        "        # save the model\n",
        "        self.model.save(saved_model_path)\n",
        "\n",
        "\n",
        "\n",
        "    # play the game\n",
        "    def play(self, saved_model_path = \"deep_Q_learning_model.h5\"):\n",
        "        # load the model\n",
        "        self.model = tf.keras.models.load_model(saved_model_path)\n",
        "\n",
        "        MAX_ITER = 100\n",
        "        iter = 0\n",
        "        position = (0, 0)\n",
        "        fin = False\n",
        "\n",
        "        while iter < MAX_ITER and not fin:\n",
        "            iter += 1\n",
        "            # update the vector of states\n",
        "            self.update_vec_etat(position)\n",
        "            # choose an action\n",
        "            action = self.choose_action(position, 0)\n",
        "            # apply the action\n",
        "            new_position, reward, fin = applicaion_action(action, position, self.space)\n",
        "\n",
        "            print( str(position) + \" (\" + str(action) + \") \" + str(new_position))\n",
        "            if iter % 10 == 0:\n",
        "                print()\n",
        "            \n",
        "            if fin:\n",
        "                print(\"fin de partie en\", iter, \"coups\")\n",
        "                break\n",
        "            # update the position\n",
        "            position = new_position\n",
        "\n",
        "        if iter == MAX_ITER:\n",
        "            print(\"Trop d'itérations\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set rewards \n",
        "Rewards.set_reward(character=\"D\", reward=-100)\n",
        "Rewards.set_reward(character=\"_\", reward=1)\n",
        "Rewards.set_reward(character=\"J\", reward=1000)\n",
        "\n",
        "# create a game\n",
        "game = DeepGame(episodes=10)"
      ],
      "metadata": {
        "id": "C9ocj0r97kU8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uCARrbRxnEfc",
        "outputId": "910ede7b-c102-4dd6-bfa7-4189644236bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 9/10, step : 99/100"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# choix de l'optimiseur\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
        "# on va définir la fonction de perte\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "# train the model\n",
        "game.train(optimizer, loss_fn, False, \"deep_Q_learning_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABqgRfAHnEfd"
      },
      "source": [
        "#### Après apprentissage on peut jouer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wZDTMhXbnEfd",
        "outputId": "f927390a-cf00-44f6-92c5-1324d9853e5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0) (DROITE) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "(0, 1) (HAUT) (0, 1)\n",
            "\n",
            "Trop d'itérations\n"
          ]
        }
      ],
      "source": [
        "# play the game\n",
        "game.play(\"deep_Q_learning_model.h5\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4428cbe1ba9314b3551257500664b995dcc328d303584ff4cad6f1a703111ed9"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}