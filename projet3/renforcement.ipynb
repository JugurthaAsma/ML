{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Développement d'un jeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "GAMMA = 0.96\n",
    "ALPHA = 0.81\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the space\n",
    "\"\"\"\n",
    "by default the default space is used\n",
    "the space is a 2D array 4 * 4 of characters\n",
    "the characters are: \n",
    "    'S' : the starting point\n",
    "    '_' : empty space\n",
    "    'J' : the goal\n",
    "    'D' : a dragon\n",
    "the default space is:\n",
    "    S___\n",
    "    D_D_ \n",
    "    ___D\n",
    "    _D_J\n",
    "\"\"\"\n",
    "\n",
    "# the default space\n",
    "default_space = [\n",
    "        ['S', '_', '_', '_'],\n",
    "        ['D', '_', 'D', '_'],\n",
    "        ['_', '_', '_', 'D'],\n",
    "        ['_', 'D', '_', 'J']\n",
    "    ]\n",
    "\n",
    "def get_default_space():\n",
    "    return default_space\n",
    "\n",
    "# a random space with number of lines and columns and a number of dragons\n",
    "def get_random_space(lines, columns, dragons):\n",
    "    space = []\n",
    "    for l in range(lines):\n",
    "        space.append([])\n",
    "        for c in range(columns):\n",
    "            space[l].append('_')\n",
    "\n",
    "    space[0][0] = 'S'\n",
    "    space[lines-1][columns-1] = 'J'\n",
    "    \n",
    "    i = 0\n",
    "    while i < dragons:\n",
    "        l = random.randint(0, lines-1)\n",
    "        c = random.randint(0, columns-1)\n",
    "        if space[l][c] == '_':\n",
    "            space[l][c] = 'D'\n",
    "            i += 1\n",
    "        else:\n",
    "            i -= 1\n",
    "\n",
    "    return space\n",
    "\n",
    "# pretty print the space\n",
    "def print_space(space):\n",
    "    for l in space:\n",
    "        for c in l:\n",
    "            print(c, end='| ')\n",
    "        \n",
    "        print()\n",
    "\n",
    "# get the size of the lines\n",
    "def get_lines_size(space):\n",
    "    return len(space)\n",
    "\n",
    "# get the size of the columns\n",
    "def get_columns_size(space):\n",
    "    return len(space[0])\n",
    "\n",
    "# get the size of the space\n",
    "def get_size(space):\n",
    "    return get_lines_size(space) * get_columns_size(space)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a static class for the rewards\n",
    "class Rewards:\n",
    "    # the rewards for each character\n",
    "    rewards = {\n",
    "        'S': 0,\n",
    "        '_': 0,\n",
    "        'J': 1,\n",
    "        'D': -1\n",
    "    }\n",
    "\n",
    "    # a method to get the reward of a character\n",
    "    def get_reward(character):\n",
    "        return Rewards.rewards.get(character)\n",
    "\n",
    "    # a method to set the reward \n",
    "    def set_rewards(rewards):\n",
    "        Rewards.rewards = rewards\n",
    "\n",
    "    # a method to set the reward of a character\n",
    "    def set_reward(character, reward):\n",
    "        Rewards.rewards[character] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a static class for the Directions\n",
    "class Directions:\n",
    "    # the directions\n",
    "    directions = [\"HAUT\", \"DROITE\", \"BAS\", \"GAUCHE\"]\n",
    "\n",
    "    # a method to get the size of the directions\n",
    "    def get_size():\n",
    "        return len(Directions.directions)\n",
    "\n",
    "    # a method to get the index of a direction\n",
    "    def get_index(direction):\n",
    "        return Directions.directions.index(direction)\n",
    "\n",
    "    # a method to get a random direction\n",
    "    def get_random_direction():\n",
    "        return random.choice(Directions.directions)\n",
    "\n",
    "    # a method to get the direction that maximizes the Q value\n",
    "    def get_max_direction(mat_q, state):\n",
    "        return Directions.directions[np.argmax(mat_q[state])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Q matrix\n",
    "# init the Q matrix with zeros and the size of the space and the directions length\n",
    "def init_mat_q(space):\n",
    "    return np.zeros((get_lines_size(space), get_columns_size(space), Directions.get_size()))\n",
    "\n",
    "# get the Q value of a state and a direction\n",
    "def get_q_value(mat_q, state, direction):\n",
    "    return mat_q[state][Directions.get_index(direction)]\n",
    "\n",
    "# update the Q matrix\n",
    "# according to state, action, reward, next_state, ALPHA and GAMMA\n",
    "def update_mat_q(mat_q, state, action, reward, next_state):\n",
    "    mat_q[state][Directions.get_index(action)] += ALPHA * (reward + GAMMA * np.max(mat_q[next_state]) - mat_q[state][Directions.get_index(action)])\n",
    "    return mat_q\n",
    "\n",
    "# pretty print the space\n",
    "def print_mat_q(mat_q, space):\n",
    "    def get_best_direction(l, c):\n",
    "        return Directions.directions[np.argmax(mat_q[l][c])]\n",
    "\n",
    "    for l in range(get_lines_size(space)):\n",
    "        for c in range(get_columns_size(space)):\n",
    "            case      = space[l][c]\n",
    "            direction = get_best_direction(l, c)\n",
    "            q_value   = str(round(get_q_value(mat_q, (l, c), direction), 2)).ljust(6)\n",
    "\n",
    "            content = case + \" (\" + q_value + \") \" + direction\n",
    "            print(content.ljust(20), end='| ')\n",
    "        print(\"\\n_______________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isWin (space, position):\n",
    "    (l,c) = position\n",
    "\n",
    "    if(space[l][c]== 'J'):\n",
    "        return True\n",
    "\n",
    "# a method to apply an action to the player\n",
    "# returns [position, reward, fin]\n",
    "def applicaion_action(action, position, space):\n",
    "\n",
    "    # reward -1 every time\n",
    "    reward = -1\n",
    "\n",
    "    (l, c) = position\n",
    "    nextPos = position\n",
    "\n",
    "    if action == \"HAUT\":\n",
    "        nextPos = (l-1,c)\n",
    "    elif action == \"DROITE\":\n",
    "        nextPos = (l,c+1)\n",
    "    elif action == \"BAS\":\n",
    "        nextPos = (l+1,c);\n",
    "    elif action == \"GAUCHE\":\n",
    "        nextPos = (l,c-1);\n",
    "\n",
    "    # check if the next position is in the space\n",
    "    if (nextPos[0] < len(space) and nextPos[1] < len(space) and nextPos[0] >=0 and nextPos[1] >=0 ):\n",
    "        position = nextPos\n",
    "\n",
    "        # back to the starting point if a dragon is encountered\n",
    "        # get the current case in the space\n",
    "        case = space[position[0]][position[1]]\n",
    "        if case == 'D':\n",
    "            position = (0, 0)\n",
    "    \n",
    "        # set the reward\n",
    "        reward += Rewards.get_reward(case)\n",
    "\n",
    "    # check if the player is at the goal\n",
    "    fin = isWin(space, position)\n",
    "\n",
    "    # if the player is at the goal, back to the starting point\n",
    "    if fin:\n",
    "        position = (0, 0)\n",
    "\n",
    "    return [position, reward, fin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Développement du Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class for the game\n",
    "class Game:\n",
    "    # constructor that takes :\n",
    "    # number of episodes : 10000 by default\n",
    "    # number of steps : 100 by default\n",
    "    # is_random_space : False by default\n",
    "    # a Q matrix : initialized with zeros (with the size of the space and the number of directions)\n",
    "    def __init__(self, episodes = 10000, steps = 100, is_random_space = False, mat_q = None):\n",
    "        self.episodes = episodes\n",
    "        self.steps = steps\n",
    "        # the space\n",
    "        if is_random_space:\n",
    "            self.space = get_random_space(4, 4, 3)\n",
    "        else:\n",
    "            self.space = get_default_space()\n",
    "        \n",
    "        # the Q matrix \n",
    "        if mat_q is None:\n",
    "            self.mat_q = init_mat_q(self.space)\n",
    "\n",
    "    # a method to choose an action with the epsilon greedy policy\n",
    "    def choose_action(self, state, epsilon, mat_q):\n",
    "        if random.random() < epsilon:\n",
    "            return Directions.get_random_direction()\n",
    "        else:\n",
    "            return Directions.get_max_direction(mat_q, state)\n",
    "\n",
    "    # a method to play one step (with mat_q, state, epsilon)\n",
    "    def oneStep(self, mat_q, state, epsilon, verbose):\n",
    "        # choose an action\n",
    "        action = self.choose_action(state, epsilon, mat_q)\n",
    "        if verbose:\n",
    "            print(action, end=', ')\n",
    "        # apply the action\n",
    "        new_state, reward, fin = applicaion_action(action, state, self.space)\n",
    "        # update the Q matrix\n",
    "        new_q = update_mat_q(mat_q, state, action, reward, new_state)\n",
    "        return new_q, new_state, fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAY\n",
    "game = Game()\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "# apply the algorithm \n",
    "for episode in range(game.episodes):\n",
    "    # reset the position\n",
    "    position = (0, 0)\n",
    "    # calculate the epsilon\n",
    "    epsilon = game.episodes / (game.episodes + episode)\n",
    "    #print(\"epsilon : \", epsilon)\n",
    "\n",
    "    # play the game\n",
    "    for step in range(1, game.steps):\n",
    "        # play one step\n",
    "        game.mat_q, position, fin = game.oneStep(game.mat_q, position, epsilon, False)\n",
    "        #print(\"position : \", game.position)\n",
    "\n",
    "        # if the game is finished\n",
    "        if fin:\n",
    "            total_steps += step\n",
    "            break\n",
    "\n",
    "print(\"total steps : \", total_steps)\n",
    "print(\"average steps : \", total_steps / game.episodes)\n",
    "\n",
    "print(game.mat_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play a with the optimal policy\n",
    "for step in range(1, game.steps):\n",
    "    # play one step\n",
    "    game.mat_q, position, fin = game.oneStep(game.mat_q, position, 0, True)\n",
    "    if fin:\n",
    "        print(\"fin de partie en\", step, \"coups\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mat_q(game.mat_q, game.space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test avec une structure 2 couches denses ayant 16 entrées (nombre de cases) et 4 sorties (4 actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class for the game\n",
    "class DeepGame:\n",
    "    # constructor that takes :\n",
    "    # number of episodes : 10000 by default\n",
    "    # number of steps : 100 by default\n",
    "    # is_random_space : False by default\n",
    "    # vec_etat : vector of states (for deep Q learning)\n",
    "    # a model : CNN\n",
    "    def __init__(self, episodes = 10000, steps = 100, is_random_space = False, vec_etat = None, model = None):\n",
    "        self.episodes = episodes\n",
    "        self.steps = steps\n",
    "        # the space\n",
    "        if is_random_space:\n",
    "            self.space = get_random_space(4, 4, 3)\n",
    "        else:\n",
    "            self.space = get_default_space()\n",
    "\n",
    "        # the vector of states\n",
    "        if vec_etat is None:\n",
    "            self.set_default_vec_etat()\n",
    "\n",
    "        # the model\n",
    "        if model is None:\n",
    "            self.set_default_model()\n",
    "\n",
    "\n",
    "    # a method to set the vector of states\n",
    "    def set_vec_etat(self, vec_etat):\n",
    "        self.vec_etat = vec_etat\n",
    "\n",
    "    # a method to set the default vector of states\n",
    "    def set_default_vec_etat(self):\n",
    "        self.vec_etat = np.zeros((1, 16))\n",
    "        self.vec_etat[0][0] = 1\n",
    "\n",
    "    # a method to reset the vector of states\n",
    "    def reset_vec_etat(self):\n",
    "        self.vec_etat = np.zeros((1, 16))\n",
    "\n",
    "    # a method to update the vector of states\n",
    "    def update_vec_etat(self, state):\n",
    "        self.reset_vec_etat()\n",
    "        self.vec_etat[0, int(get_lines_size(self.space) * state[0] + state[1])] = 1\n",
    "\n",
    "    # a method to set the model\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    # a method to set the default model\n",
    "    def set_default_model(self):\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(4, activation='relu', input_shape=[16]),\n",
    "            tf.keras.layers.Dense(4, activation='relu'),\n",
    "            tf.keras.layers.Dense(4),\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # a method to choose an action with the epsilon greedy policy\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return Directions.get_random_direction()\n",
    "        else:\n",
    "            Sortie_Q = self.model.predict(self.vec_etat, verbose=0)  # En entrée le vecteur symbolisant l'état\n",
    "            return Directions.directions[np.argmax(Sortie_Q)] #On sélectionne l'action associée avec la sortie max\n",
    "     \n",
    "    # Train the model\n",
    "    def train(self, optimizer, loss_fn, verbose, saved_model_path = \"deep_Q_learning_model.h5\"):\n",
    "        # create a stable model\n",
    "        model_stable = tf.keras.models.clone_model(self.model)\n",
    "        model_stable.set_weights(self.model.get_weights())\n",
    "\n",
    "        history = np.zeros(self.episodes)\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            # reset the position\n",
    "            position = (0, 0)\n",
    "            # calculate the epsilon\n",
    "            epsilon = self.episodes / (self.episodes + episode)\n",
    "\n",
    "            for step in range(1, self.steps):\n",
    "                # play one step\n",
    "                # choose an action\n",
    "                action = self.choose_action(position, epsilon, self.model)\n",
    "                # apply the action\n",
    "                new_position, reward, fin = applicaion_action(action, position, self.space)\n",
    "                if verbose:\n",
    "                    print(action, end=\", \")\n",
    "\n",
    "                if fin:\n",
    "                    if verbose:\n",
    "                        print(\"fin de partie en\", step, \"coups\")\n",
    "                    break\n",
    "\n",
    "                # set weights of the stable model\n",
    "                if step % 10 == 0:\n",
    "                    model_stable.set_weights(self.model.get_weights())\n",
    "\n",
    "                vec_etat_next = np.zeros((1, 16)) # ca sera l'entree du reseau\n",
    "                vec_etat_next[0, int(get_lines_size(self.space) * new_position[0] + new_position[1])] = 1\n",
    "\n",
    "                # model stable predict\n",
    "                next_Q = model_stable.predict(vec_etat_next, verbose=0)\n",
    "                next_Q_max = np.max(next_Q)\n",
    "\n",
    "                # target\n",
    "                target = reward + GAMMA * next_Q_max * (1 - fin)\n",
    "\n",
    "                # gradient descent\n",
    "                with tf.GradientTape() as tape:\n",
    "                    predict = self.model.predict(self.vec_etat, verbose=0) # ce que l'on pense obtenir\n",
    "                    mask = tf.one_hot(action, Directions.get_size())\n",
    "                    val_predict = tf.reduce_sum(predict * mask, axis=1)\n",
    "                    loss = loss_fn(target, val_predict)\n",
    "\n",
    "                gradients = tape.gradient(loss, self.model.trainable_variables) #calcul du gradient de la focntion loss en fonction des variables du modèle \n",
    "                optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) # optimisation des paramètres du modèle\n",
    "                history[episode] = loss.numpy() # on récupère la valeur pour afficher l'évolution de l'erreur\n",
    "\n",
    "                # update the position\n",
    "                position = new_position\n",
    "\n",
    "        # save the model\n",
    "        self.model.save(saved_model_path)\n",
    "\n",
    "\n",
    "\n",
    "    # play the game\n",
    "    def play(self, saved_model_path = \"deep_Q_learning_model.h5\"):\n",
    "        # load the model\n",
    "        self.model = tf.keras.models.load_model(saved_model_path)\n",
    "\n",
    "        MAX_ITER = 1000\n",
    "        iter = 0\n",
    "        position = (0, 0)\n",
    "        fin = False\n",
    "\n",
    "        while iter < MAX_ITER and not fin:\n",
    "            iter += 1\n",
    "            # update the vector of states\n",
    "            self.update_vec_etat(position)\n",
    "            # choose an action\n",
    "            action = self.choose_action(position, 0, self.model)\n",
    "            # apply the action\n",
    "            new_position, reward, fin = applicaion_action(action, position, self.space)\n",
    "            print(action, end=\", \")\n",
    "            if fin:\n",
    "                print(\"fin de partie en\", iter, \"coups\")\n",
    "                break\n",
    "            # update the position\n",
    "            position = new_position\n",
    "\n",
    "        if iter == MAX_ITER:\n",
    "            print(\"Trop d'itérations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix de l'optimiseur\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01) \n",
    "\n",
    "# on va définir la fonction de perte\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# on va définir la dérivé de la fonction d'activation\n",
    "\n",
    "# relu\n",
    "@tf.custom_gradient\n",
    "def my_relu(x):\n",
    "    y = tf.nn.relu(x)\n",
    "    def grad(dy):\n",
    "        return dy * tf.cast(x > 0, tf.float32)\n",
    "    return y, grad\n",
    "\n",
    "# tanh\n",
    "@tf.custom_gradient\n",
    "def my_tanh(x):\n",
    "    y = tf.math.tanh(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - y ** 2)\n",
    "    return y, grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a game\n",
    "game = DeepGame()\n",
    "\n",
    "# train the model\n",
    "game.train(optimizer, loss_fn, False, \"deep_Q_learning_model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Après apprentissage on peut jouer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play the game\n",
    "game.play(\"deep_Q_learning_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4428cbe1ba9314b3551257500664b995dcc328d303584ff4cad6f1a703111ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
