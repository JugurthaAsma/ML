{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Développement d'un jeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "GAMMA = 0.96\n",
    "ALPHA = 0.81\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the space\n",
    "\"\"\"\n",
    "by default the default space is used\n",
    "the space is a 2D array 4 * 4 of characters\n",
    "the characters are: \n",
    "    'S' : the starting point\n",
    "    '_' : empty space\n",
    "    'J' : the goal\n",
    "    'D' : a dragon\n",
    "the default space is:\n",
    "    S___\n",
    "    D_D_ \n",
    "    ___D\n",
    "    _D_J\n",
    "\"\"\"\n",
    "def get_default_space():\n",
    "    return [\n",
    "        ['S', '_', '_', '_'],\n",
    "        ['D', '_', 'D', '_'],\n",
    "        ['_', '_', '_', 'D'],\n",
    "        ['_', 'D', '_', 'J']\n",
    "    ]\n",
    "\n",
    "# a random space with number of lines and columns and a number of dragons\n",
    "def get_random_space(lines, columns, dragons):\n",
    "    space = []\n",
    "    for l in range(lines):\n",
    "        space.append([])\n",
    "        for c in range(columns):\n",
    "            space[l].append('_')\n",
    "\n",
    "    space[0][0] = 'S'\n",
    "    space[lines-1][columns-1] = 'J'\n",
    "    \n",
    "    i = 0\n",
    "    while i < dragons:\n",
    "        l = random.randint(0, lines-1)\n",
    "        c = random.randint(0, columns-1)\n",
    "        if space[l][c] == '_':\n",
    "            space[l][c] = 'D'\n",
    "            i += 1\n",
    "        else:\n",
    "            i -= 1\n",
    "\n",
    "    return space\n",
    "\n",
    "# pretty print the space\n",
    "def print_space(space):\n",
    "    for l in space:\n",
    "        for c in l:\n",
    "            print(c, end='| ')\n",
    "        \n",
    "        print()\n",
    "\n",
    "# get the size of the lines\n",
    "def get_lines_size(space):\n",
    "    return len(space)\n",
    "\n",
    "# get the size of the columns\n",
    "def get_columns_size(space):\n",
    "    return len(space[0])\n",
    "\n",
    "# get the size of the space\n",
    "def get_size(space):\n",
    "    return get_lines_size(space) * get_columns_size(space)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a static class for the rewards\n",
    "class Rewards:\n",
    "    # the rewards for each character\n",
    "    rewards = {\n",
    "        'S': 0,\n",
    "        '_': 0,\n",
    "        'J': 1,\n",
    "        'D': -1\n",
    "    }\n",
    "\n",
    "    # a method to get the reward of a character\n",
    "    def get_reward(character):\n",
    "        return Rewards.rewards.get(character)\n",
    "\n",
    "    # a method to set the reward \n",
    "    def set_rewards(rewards):\n",
    "        Rewards.rewards = rewards\n",
    "\n",
    "    # a method to set the reward of a character\n",
    "    def set_reward(character, reward):\n",
    "        Rewards.rewards[character] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a static class for the Directions\n",
    "class Directions:\n",
    "    # the directions\n",
    "    directions = [\"HAUT\", \"DROITE\", \"BAS\", \"GAUCHE\"]\n",
    "\n",
    "    # a method to get the size of the directions\n",
    "    def get_size():\n",
    "        return len(Directions.directions)\n",
    "\n",
    "    # a method to get the index of a direction\n",
    "    def get_index(direction):\n",
    "        return Directions.directions.index(direction)\n",
    "\n",
    "    # a method to get a random direction\n",
    "    def get_random_direction():\n",
    "        return random.choice(Directions.directions)\n",
    "\n",
    "    # a method to get the direction that maximizes the Q value\n",
    "    def get_max_direction(mat_q, state):\n",
    "        return Directions.directions[np.argmax(mat_q[state])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Q matrix\n",
    "# init the Q matrix with zeros and the size of the space and the directions length\n",
    "def init_mat_q(space):\n",
    "    return np.zeros((get_lines_size(space), get_columns_size(space), Directions.get_size()))\n",
    "\n",
    "# get the Q value of a state and a direction\n",
    "def get_q_value(mat_q, state, direction):\n",
    "    return mat_q[state][Directions.get_index(direction)]\n",
    "\n",
    "# update the Q matrix\n",
    "# according to state, action, reward, next_state, ALPHA and GAMMA\n",
    "def update_mat_q(mat_q, state, action, reward, next_state):\n",
    "    mat_q[state][Directions.get_index(action)] += ALPHA * (reward + GAMMA * np.max(mat_q[next_state]) - mat_q[state][Directions.get_index(action)])\n",
    "    return mat_q\n",
    "\n",
    "# pretty print the space\n",
    "def print_mat_q(mat_q, space):\n",
    "    def get_best_direction(l, c):\n",
    "        return Directions.directions[np.argmax(mat_q[l][c])]\n",
    "\n",
    "    for l in range(get_lines_size(space)):\n",
    "        for c in range(get_columns_size(space)):\n",
    "            case      = space[l][c]\n",
    "            direction = get_best_direction(l, c)\n",
    "            q_value   = str(round(get_q_value(mat_q, (l, c), direction), 2)).ljust(6)\n",
    "\n",
    "            content = case + \" (\" + q_value + \") \" + direction\n",
    "            print(content.ljust(20), end='| ')\n",
    "        print(\"\\n_______________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isWin (space, position):\n",
    "    (l,c) = position\n",
    "\n",
    "    if(space[l][c]== 'J'):\n",
    "        return True\n",
    "\n",
    "# a method to apply an action to the player\n",
    "# returns [position, reward, fin]\n",
    "def applicaion_action(action, position, space):\n",
    "\n",
    "    # reward -1 every time\n",
    "    reward = -1\n",
    "\n",
    "    (l, c) = position\n",
    "    nextPos = position\n",
    "\n",
    "    if action == \"HAUT\":\n",
    "        nextPos = (l-1,c)\n",
    "    elif action == \"DROITE\":\n",
    "        nextPos = (l,c+1)\n",
    "    elif action == \"BAS\":\n",
    "        nextPos = (l+1,c);\n",
    "    elif action == \"GAUCHE\":\n",
    "        nextPos = (l,c-1);\n",
    "\n",
    "    # check if the next position is in the space\n",
    "    if (nextPos[0] < len(space) and nextPos[1] < len(space) and nextPos[0] >=0 and nextPos[1] >=0 ):\n",
    "        position = nextPos\n",
    "\n",
    "        # back to the starting point if a dragon is encountered\n",
    "        # get the current case in the space\n",
    "        case = space[position[0]][position[1]]\n",
    "        if case == 'D':\n",
    "            position = (0, 0)\n",
    "    \n",
    "        # set the reward\n",
    "        reward += Rewards.get_reward(case)\n",
    "\n",
    "    # check if the player is at the goal\n",
    "    fin = isWin(space, position)\n",
    "\n",
    "    # if the player is at the goal, back to the starting point\n",
    "    if fin:\n",
    "        position = (0, 0)\n",
    "\n",
    "    return [position, reward, fin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Développement du Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class for the game\n",
    "class Game:\n",
    "    # constructor that takes :\n",
    "    # number of episodes : 10000 by default\n",
    "    # number of steps : 100 by default\n",
    "    # is_random_space : False by default\n",
    "    # a Q matrix : initialized with zeros (with the size of the space and the number of directions)\n",
    "    def __init__(self, episodes = 10000, steps = 100, is_random_space = False):\n",
    "        self.episodes = episodes\n",
    "        self.steps = steps\n",
    "        # the space\n",
    "        if is_random_space:\n",
    "            self.space = get_random_space(4, 4, 3)\n",
    "        else:\n",
    "            self.space = get_default_space()\n",
    "    \n",
    "        # the Q matrix \n",
    "        self.mat_q = init_mat_q(self.space)\n",
    "\n",
    "    # a method to choose an action with the epsilon greedy policy\n",
    "    def choose_action(self, state, epsilon, mat_q):\n",
    "        if random.random() < epsilon:\n",
    "            return Directions.get_random_direction()\n",
    "        else:\n",
    "            return Directions.get_max_direction(mat_q, state)\n",
    "\n",
    "    # a method to play one step (with mat_q, state, epsilon)\n",
    "    def oneStep(self, mat_q, state, epsilon, verbose):\n",
    "        # choose an action\n",
    "        action = self.choose_action(state, epsilon, mat_q)\n",
    "        if verbose:\n",
    "            print(action, end=', ')\n",
    "        # apply the action\n",
    "        new_state, reward, fin = applicaion_action(action, state, self.space)\n",
    "        # update the Q matrix\n",
    "        new_q = update_mat_q(mat_q, state, action, reward, new_state)\n",
    "        return new_q, new_state, fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total steps :  238132\n",
      "average steps :  23.8132\n",
      "[[[-21.396843   -21.24671146 -22.396843   -21.396843  ]\n",
      "  [-21.24671146 -21.396843   -21.09032444 -21.396843  ]\n",
      "  [-21.396843   -21.54096928 -22.396843   -21.24671146]\n",
      "  [-21.54096928 -21.54096928 -21.67933051 -21.396843  ]]\n",
      "\n",
      " [[  0.           0.           0.           0.        ]\n",
      "  [-21.24671146 -22.396843   -20.92742129 -22.396843  ]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [-21.54096928 -21.67933051 -22.396843   -22.396843  ]]\n",
      "\n",
      " [[-22.396843   -20.92742129 -21.24671146 -21.09032444]\n",
      "  [-21.09032444 -20.75773051 -22.396843   -21.09032444]\n",
      "  [-22.396843   -22.396843   -20.58096928 -20.92742129]\n",
      "  [  0.           0.           0.           0.        ]]\n",
      "\n",
      " [[-21.09032444 -22.396843   -21.24671146 -21.24671146]\n",
      "  [  0.           0.           0.           0.        ]\n",
      "  [-20.75773051 -20.396843   -20.58096928 -22.396843  ]\n",
      "  [  0.           0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# PLAY\n",
    "game = Game()\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "# apply the algorithm \n",
    "for episode in range(game.episodes):\n",
    "    # reset the position\n",
    "    position = (0, 0)\n",
    "    # calculate the epsilon\n",
    "    epsilon = game.episodes / (game.episodes + episode)\n",
    "    #print(\"epsilon : \", epsilon)\n",
    "\n",
    "    # play the game\n",
    "    for step in range(1, game.steps):\n",
    "        # play one step\n",
    "        game.mat_q, position, fin = game.oneStep(game.mat_q, position, epsilon, False)\n",
    "        #print(\"position : \", game.position)\n",
    "\n",
    "        # if the game is finished\n",
    "        if fin:\n",
    "            total_steps += step\n",
    "            break\n",
    "\n",
    "print(\"total steps : \", total_steps)\n",
    "print(\"average steps : \", total_steps / game.episodes)\n",
    "\n",
    "print(game.mat_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROITE, BAS, BAS, DROITE, BAS, DROITE, fin de partie en 6 coups\n"
     ]
    }
   ],
   "source": [
    "# play a with the optimal policy\n",
    "for step in range(1, game.steps):\n",
    "    # play one step\n",
    "    game.mat_q, position, fin = game.oneStep(game.mat_q, position, 0, True)\n",
    "    if fin:\n",
    "        print(\"fin de partie en\", step, \"coups\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S (-21.25) DROITE   | _ (-21.09) BAS      | _ (-21.25) GAUCHE   | _ (-21.4 ) GAUCHE   | \n",
      "_______________________________________________________________________________________\n",
      "D (0.0   ) HAUT     | _ (-20.93) BAS      | D (0.0   ) HAUT     | _ (-21.54) HAUT     | \n",
      "_______________________________________________________________________________________\n",
      "_ (-20.93) DROITE   | _ (-20.76) DROITE   | _ (-20.58) BAS      | D (0.0   ) HAUT     | \n",
      "_______________________________________________________________________________________\n",
      "_ (-21.09) HAUT     | D (0.0   ) HAUT     | _ (-20.4 ) DROITE   | J (0.0   ) HAUT     | \n",
      "_______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print_mat_q(game.mat_q, game.space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sys\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test avec une structure 2 couches denses ayant 16 entrées (nombre de cases) et 4 sorties (4 actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 modifier la fonction choose_action avec en sortir model.predict(vec_etat)\n",
    "def choose_action(vect_etat, epsilon, model):\n",
    "    # L'agent est dans un certain état s, on choisit une action a selon :\n",
    "\n",
    "    # Au hasard avec une probabilité epsilon\n",
    "    if random.random() < epsilon:\n",
    "        # on choisit une action aléatoire\n",
    "        action = random.choice(DIRECTIONS)\n",
    "    else:\n",
    "        # La meilleure avec une probabilité 1-epsilon\n",
    "        Sortie_Q = model.predict(vect_etat, verbose=0)  # En entrée le vecteur symbolisant l'état\n",
    "        action = DIRECTIONS[np.argmax(Sortie_Q)] #On sélectionne l'action associée avec la sortie max\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation du model\n",
    "# Une structure simple avec 16 entrées et 4 sorties, la sortie est sans activation\n",
    "model = Sequential([\n",
    "    Dense(4, activation='relu', input_shape=[16]),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(4),\n",
    "])\n",
    "\n",
    "# En préambule création d’un second modèle\n",
    "model_stable = keras.models.clone_model(model)\n",
    "model_stable.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix de l'optimiseur\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01) \n",
    "\n",
    "# on va définir la fonction de perte\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# on va définir la dérivé de la fonction d'activation\n",
    "\n",
    "# relu\n",
    "@tf.custom_gradient\n",
    "def my_relu(x):\n",
    "    y = tf.nn.relu(x)\n",
    "    def grad(dy):\n",
    "        return dy * tf.cast(x > 0, tf.float32)\n",
    "    return y, grad\n",
    "\n",
    "# tanh\n",
    "@tf.custom_gradient\n",
    "def my_tanh(x):\n",
    "    y = tf.math.tanh(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - y ** 2)\n",
    "    return y, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function d'initialisation\n",
    "def init():\n",
    "    position = (0,0)\n",
    "    # Créer en entrée un vecteur de taille lines * columns avec comme nombre de sorties le nombre d'action possible \n",
    "    vect_etat = np.zeros((1,16))\n",
    "    vect_etat[0, int(len(space) * position[0] + position[1])] = 1\n",
    "\n",
    "    return position, vect_etat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position, vect_etat = init()\n",
    "\n",
    "# ITERATIONS D'APPRENTISSAGE\n",
    "\n",
    "# on applique l'algorithme\n",
    "for iterationPartie in range(Nparties):\n",
    "    progress = \"Partie \" + str(iterationPartie) + \"/\" + str(Nparties) \n",
    "    sys.stdout.write(\"\\r\" + progress)\n",
    "\n",
    "    # on réinitialise la position\n",
    "    position = (0,0)\n",
    "\n",
    "    # calcul de epsilon\n",
    "    epsilon = Nparties/(Nparties+iterationPartie)\n",
    "\n",
    "    fin = False\n",
    "\n",
    "    while not fin:\n",
    "        # on choisit une action\n",
    "        action = choose_action(vect_etat, epsilon, model)\n",
    "\n",
    "        # on applique l'action\n",
    "        new_position, reward, fin = application_action(action, position, space)\n",
    "\n",
    "        # on met à jour vect_etat\n",
    "        vect_etat[0, int(len(space) * position[0] + position[1])] = 0\n",
    "        vect_etat[0, int(len(space) * new_position[0] + new_position[1])] = 1\n",
    "\n",
    "        # model stable\n",
    "        sortie_Q_stable = model_stable.predict(vect_etat, verbose=0)\n",
    "        max_Q = np.max(sortie_Q_stable)\n",
    "\n",
    "        target = reward + gamma * max_Q\n",
    "\n",
    "        # descente de gradient\n",
    "        with tf.GradientTape() as tape:\n",
    "            predict = model(vect_etat, training=True)  #Ce que l'on pense obtenir \n",
    "            mask = tf.one_hot(DIRECTIONS.index(action), len(DIRECTIONS)) #On crée un masque pour sélectionner la sortie correspondant à l'action choisie\n",
    "            val_predict = tf.reduce_sum(predict * mask, axis=1) #On sélectionne la sortie correspondant à l'action choisie\n",
    "            loss = loss_fn(target, val_predict) #On calcule la perte\n",
    "        \n",
    "        # on applique la descente de gradient\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # optimisation des paramètres\n",
    "\n",
    "        # on met à jour la position\n",
    "        position = new_position\n",
    "\n",
    "        # on met à jour le model stable\n",
    "        if iterationPartie % 100 == 0:\n",
    "            model_stable.set_weights(model.get_weights())\n",
    "\n",
    "    \n",
    "# sauvegarde du model\n",
    "model.save('deep_Q_learning_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JEU\n",
    "\n",
    "def play():\n",
    "\n",
    "    # affichage de l'espace avec un retour à la ligne à la fin de chaque ligne\n",
    "    for i in range(len(space)):\n",
    "        for j in range(len(space)):\n",
    "            print(space[i][j], end=\"| \")\n",
    "        print()\n",
    "    # on charge le model\n",
    "    model = keras.models.load_model('deep_Q_learning_model.h5')\n",
    "\n",
    "    # initialisation de la position\n",
    "    position = (0,0)\n",
    "    fin = False\n",
    "\n",
    "    MAX_ITER = 100\n",
    "    iter = 0\n",
    "    while not fin and iter < MAX_ITER:\n",
    "        iter += 1\n",
    "\n",
    "        # on crée le vecteur d'état\n",
    "        vect_etat = np.zeros((1,16))\n",
    "        vect_etat[0, int(len(space) * position[0] + position[1])] = 1\n",
    "\n",
    "        # on choisit une action\n",
    "        action = choose_action(vect_etat, 0, model)\n",
    "\n",
    "        # on applique l'action\n",
    "        new_position, reward, fin = application_action(action, position, space)\n",
    "\n",
    "        # on met à jour la position\n",
    "        position = new_position\n",
    "\n",
    "        # on affiche l'action choisie avec \", \" en end de print pour ne pas faire de retour à la ligne\n",
    "        # mais avec un retour à la ligne tout les 10 actions\n",
    "        print(action, end=\", \")\n",
    "        if iter % 10 == 0:\n",
    "            print()\n",
    "        \n",
    "\n",
    "\n",
    "    if fin:\n",
    "        print(\"Victoire ! en \" + str(iter) + \" itérations\")\n",
    "    else:\n",
    "        print(\"Défaite\")\n",
    "        if (iter >= MAX_ITER):\n",
    "            print(\"Trop d'itérations\")\n",
    "\n",
    "play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4428cbe1ba9314b3551257500664b995dcc328d303584ff4cad6f1a703111ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
