{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5HGSGxhnEfS"
      },
      "source": [
        "# 1. Développement d'un jeu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "16u0v0mvnEfU"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "6bQNPZzonEfV"
      },
      "outputs": [],
      "source": [
        "# configuration\n",
        "GAMMA = 0.96\n",
        "ALPHA = 0.81"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehrDGUS-pwUG"
      },
      "source": [
        "##### Créer une fonction qui permet de simuler le plateau en positionnant notamment les différents éléments (case départ, fin, dragons)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "dWZoG4tanEfW"
      },
      "outputs": [],
      "source": [
        "#for the space\n",
        "\"\"\"\n",
        "by default the default space is used\n",
        "the space is a 2D array 4 * 4 of characters\n",
        "the characters are: \n",
        "    'S' : the starting point\n",
        "    '_' : empty space\n",
        "    'J' : the goal\n",
        "    'D' : a dragon\n",
        "the default space is:\n",
        "    S___\n",
        "    D_D_ \n",
        "    ___D\n",
        "    _D_J\n",
        "\"\"\"\n",
        "\n",
        "# the default space\n",
        "default_space = [\n",
        "        # ['S', '_', '_', '_'],\n",
        "        # ['D', '_', 'D', '_'],\n",
        "        # ['_', '_', '_', 'D'],\n",
        "        # ['_', 'D', '_', 'J']\n",
        "\n",
        "        ['S',  '_'],\n",
        "        ['D',  'J']\n",
        "    ]\n",
        "\n",
        "def get_default_space():\n",
        "    return default_space\n",
        "\n",
        "# a random space with number of lines and columns and a number of dragons\n",
        "def get_random_space(lines, columns, dragons):\n",
        "    space = []\n",
        "    for l in range(lines):\n",
        "        space.append([])\n",
        "        for c in range(columns):\n",
        "            space[l].append('_')\n",
        "\n",
        "    space[0][0] = 'S'\n",
        "    space[lines-1][columns-1] = 'J'\n",
        "    \n",
        "    i = 0\n",
        "    while i < dragons:\n",
        "        l = random.randint(0, lines-1)\n",
        "        c = random.randint(0, columns-1)\n",
        "        if space[l][c] == '_':\n",
        "            space[l][c] = 'D'\n",
        "            i += 1\n",
        "        else:\n",
        "            i -= 1\n",
        "\n",
        "    return space\n",
        "\n",
        "# pretty print the space\n",
        "def print_space(space):\n",
        "    for l in space:\n",
        "        for c in l:\n",
        "            print(c, end='| ')\n",
        "        \n",
        "        print()\n",
        "\n",
        "# get the size of the lines\n",
        "def get_lines_size(space):\n",
        "    return len(space)\n",
        "\n",
        "# get the size of the columns\n",
        "def get_columns_size(space):\n",
        "    return len(space[0])\n",
        "\n",
        "# get the size of the space\n",
        "def get_size(space):\n",
        "    return get_lines_size(space) * get_columns_size(space)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "-eVBWhkOnEfX"
      },
      "outputs": [],
      "source": [
        "# a static class for the rewards\n",
        "class Rewards:\n",
        "    # the rewards for each character\n",
        "    rewards = {\n",
        "        'S': 0,\n",
        "        '_': 0,\n",
        "        'J': 1,\n",
        "        'D': -1\n",
        "    }\n",
        "\n",
        "    # a method to get the reward of a character\n",
        "    def get_reward(character):\n",
        "        return Rewards.rewards.get(character)\n",
        "\n",
        "    # a method to set the reward \n",
        "    def set_rewards(rewards):\n",
        "        Rewards.rewards = rewards\n",
        "\n",
        "    # a method to set the reward of a character\n",
        "    def set_reward(character, reward):\n",
        "        Rewards.rewards[character] = reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qL0SIBRpwUI"
      },
      "source": [
        "##### Créer une fonction qui permet de simuler l'interaction entre l'agent et son environnement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "amIQUyAKnEfX"
      },
      "outputs": [],
      "source": [
        "# a static class for the Directions\n",
        "class Directions:\n",
        "    # the directions\n",
        "    directions = [\"HAUT\", \"DROITE\", \"BAS\", \"GAUCHE\"]\n",
        "\n",
        "    # a method to get the size of the directions\n",
        "    def get_size():\n",
        "        return len(Directions.directions)\n",
        "\n",
        "    # a method to get the index of a direction\n",
        "    def get_index(direction):\n",
        "        return Directions.directions.index(direction)\n",
        "\n",
        "    # a method to get a random direction\n",
        "    def get_random_direction():\n",
        "        return random.choice(Directions.directions)\n",
        "\n",
        "    # a method to get the direction that maximizes the Q value\n",
        "    def get_max_direction(mat_q, state):\n",
        "        return Directions.directions[np.argmax(mat_q[state])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "emfLzRnwnEfY"
      },
      "outputs": [],
      "source": [
        "def isFin (space, position, verbose = False):\n",
        "    (l,c) = position\n",
        "\n",
        "    # if the player is at the goal, back to the starting point\n",
        "    if(space[l][c]== 'J'):\n",
        "        # print in green win\n",
        "        if verbose:\n",
        "            print(\"\\033[92m\" + \n",
        "            \"*************************************************************************************\\n\" +\n",
        "            \"*********************************** YOU WIN *****************************************\\n\" +\n",
        "            \"*************************************************************************************\\n\" +\n",
        "            \"\\033[0m\")\n",
        "        return True, (0,0)\n",
        "\n",
        "    # # if the player ecnounters a dragon, back to the starting point\n",
        "    if(space[l][c]== 'D'):\n",
        "        # print in red loose\n",
        "        if verbose:\n",
        "            print(\"\\033[91m\" +\n",
        "            \"*********************************** YOU LOOSE ****************************************\\n\" +\n",
        "            \"\\033[0m\")\n",
        "        return True, (0,0)\n",
        "\n",
        "    return False, position\n",
        "\n",
        "# a method to apply an action to the player\n",
        "# returns [position, reward, fin]\n",
        "def applicaion_action(action, position, space, verbose = False):\n",
        "\n",
        "    # reward -1 every time\n",
        "    reward = -1\n",
        "\n",
        "    (l, c) = position\n",
        "    nextPos = position\n",
        "\n",
        "    if action == \"HAUT\":\n",
        "        nextPos = (l-1,c)\n",
        "    elif action == \"DROITE\":\n",
        "        nextPos = (l,c+1)\n",
        "    elif action == \"BAS\":\n",
        "        nextPos = (l+1,c);\n",
        "    elif action == \"GAUCHE\":\n",
        "        nextPos = (l,c-1);\n",
        "\n",
        "    # check if the next position is in the space\n",
        "    if (nextPos[0] < len(space) and nextPos[1] < len(space) and nextPos[0] >=0 and nextPos[1] >=0 ):\n",
        "        position = nextPos\n",
        "        # get the current case in the space\n",
        "        case = space[position[0]][position[1]]\n",
        "        # set the reward\n",
        "        reward += Rewards.get_reward(case)\n",
        "\n",
        "    # check if the player is at the goal\n",
        "    fin, position = isFin(space, position, verbose)\n",
        "\n",
        "    return [position, reward, fin]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3CB8YzfpwUK"
      },
      "source": [
        "### Donner quelques éléments de commentaire sur la mise en place de l'environnement de jeux\n",
        "\n",
        "\n",
        "##### 1) Le plateau\n",
        "\n",
        "Pour créer un environnement de jeux, nous avons d'abord créé une la variable ***default_space*** qui est un tableau 2 dimensions **(4*4)** de charactères qui representent les cases du plateau de jeu proposé. Les cases sont représentées par les caractères suivants:\n",
        "\n",
        "*   ***\"_\"*** représente une case vide\n",
        "*   ***\"D\"*** représente une case avec un dragon\n",
        "*   ***\"S\"*** représente la case de départ\n",
        "*   ***\"J\"*** représente la case d'arrivée (JAIL)\n",
        "\n",
        "\n",
        "Nous avons aussi créé une fonction ***get_random_space*** qui permet de générer un plateau de jeu aléatoire en precisant le nombre de lignes, de colonnes etle nombre de dragons à placer sur le plateau.\n",
        "\n",
        "Nous avons aussi créé une fonction ***print_space*** qui permet d'afficher le plateau de jeu.\n",
        "\n",
        "##### 2) L'interaction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDxUSIRynEfZ"
      },
      "source": [
        "# 2. Développement du Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "Ug3WW4dUpwUL"
      },
      "outputs": [],
      "source": [
        "# for the Q matrix\n",
        "# init the Q matrix with zeros and the size of the space and the directions length\n",
        "def init_mat_q(space):\n",
        "    return np.zeros((get_lines_size(space), get_columns_size(space), Directions.get_size()))\n",
        "\n",
        "# get the Q value of a state and a direction\n",
        "def get_q_value(mat_q, state, direction):\n",
        "    return mat_q[state][Directions.get_index(direction)]\n",
        "\n",
        "# update the Q matrix\n",
        "# according to state, action, reward, next_state, ALPHA and GAMMA\n",
        "def update_mat_q(mat_q, state, action, reward, next_state):\n",
        "    mat_q[state][Directions.get_index(action)] += ALPHA * (reward + GAMMA * np.max(mat_q[next_state]) - mat_q[state][Directions.get_index(action)])\n",
        "    return mat_q\n",
        "\n",
        "# pretty print the space\n",
        "def print_mat_q(mat_q, space):\n",
        "    def get_best_direction(l, c):\n",
        "        return Directions.directions[np.argmax(mat_q[l][c])]\n",
        "\n",
        "    for l in range(get_lines_size(space)):\n",
        "        for c in range(get_columns_size(space)):\n",
        "            case      = space[l][c]\n",
        "            direction = get_best_direction(l, c)\n",
        "            q_value   = str(round(get_q_value(mat_q, (l, c), direction), 2)).ljust(6)\n",
        "\n",
        "            content = case + \" (\" + q_value + \") \" + direction\n",
        "            print(content.ljust(20), end='| ')\n",
        "        print(\"\\n_______________________________________________________________________________________\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "TIz5A3zznEfZ"
      },
      "outputs": [],
      "source": [
        "# a class for the game\n",
        "class Game:\n",
        "    # constructor that takes :\n",
        "    # number of episodes : 10000 by default\n",
        "    # number of steps : 100 by default\n",
        "    # is_random_space : False by default\n",
        "    # a Q matrix : initialized with zeros (with the size of the space and the number of directions)\n",
        "    def __init__(self, episodes = 10000, steps = 100, is_random_space = False, mat_q = None):\n",
        "        self.episodes = episodes\n",
        "        self.steps = steps\n",
        "        # the space\n",
        "        if is_random_space:\n",
        "            self.space = get_random_space(4, 4, 3)\n",
        "        else:\n",
        "            self.space = get_default_space()\n",
        "        \n",
        "        # the Q matrix \n",
        "        if mat_q is None:\n",
        "            self.mat_q = init_mat_q(self.space)\n",
        "\n",
        "    # a method to choose an action with the epsilon greedy policy\n",
        "    def choose_action(self, state, epsilon, mat_q):\n",
        "        if random.random() < epsilon:\n",
        "            return Directions.get_random_direction()\n",
        "        else:\n",
        "            return Directions.get_max_direction(mat_q, state)\n",
        "\n",
        "    # a method to play one step (with mat_q, state, epsilon)\n",
        "    def oneStep(self, mat_q, state, epsilon, verbose):\n",
        "        # choose an action\n",
        "        action = self.choose_action(state, epsilon, mat_q)\n",
        "        if verbose:\n",
        "            print(action, end=', ')\n",
        "        # apply the action\n",
        "        new_state, reward, fin = applicaion_action(action, state, self.space)\n",
        "        # update the Q matrix\n",
        "        new_q = update_mat_q(mat_q, state, action, reward, new_state)\n",
        "        return new_q, new_state, fin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWZfAABqnEfa",
        "outputId": "25e49b0d-8cf6-4744-cd31-e087370d865d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total steps :  33056\n",
            "average steps :  3.3056\n",
            "[[[-13.24489796 -12.75510204 -14.24489796 -13.24489796]\n",
            "  [-12.75510204 -12.75510204 -12.24489796 -13.24489796]]\n",
            "\n",
            " [[  0.           0.           0.           0.        ]\n",
            "  [  0.           0.           0.           0.        ]]]\n"
          ]
        }
      ],
      "source": [
        "# PLAY\n",
        "game = Game()\n",
        "\n",
        "total_steps = 0\n",
        "\n",
        "# apply the algorithm \n",
        "for episode in range(game.episodes):\n",
        "    # reset the position\n",
        "    position = (0, 0)\n",
        "    # calculate the epsilon\n",
        "    epsilon = game.episodes / (game.episodes + episode)\n",
        "    #print(\"epsilon : \", epsilon)\n",
        "\n",
        "    # play the game\n",
        "    for step in range(1, game.steps):\n",
        "        # play one step\n",
        "        game.mat_q, position, fin = game.oneStep(game.mat_q, position, epsilon, False)\n",
        "        # if the game is finished\n",
        "        if fin:\n",
        "            total_steps += step\n",
        "            break\n",
        "\n",
        "print(\"total steps : \", total_steps)\n",
        "print(\"average steps : \", total_steps / game.episodes)\n",
        "\n",
        "print(game.mat_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6AXQlAnnEfa",
        "outputId": "b5b499ad-8ac4-447d-c148-f4c2894e30a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DROITE, BAS, fin de partie en 2 coups\n"
          ]
        }
      ],
      "source": [
        "# play a with the optimal policy\n",
        "for step in range(1, game.steps):\n",
        "    # play one step\n",
        "    game.mat_q, position, fin = game.oneStep(game.mat_q, position, 0, True)\n",
        "    if fin:\n",
        "        print(\"fin de partie en\", step, \"coups\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rYr5rkcnEfa",
        "outputId": "bc39161b-ed17-4bdf-ad74-a48b44a6667e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S (-12.76) DROITE   | _ (-12.24) BAS      | \n",
            "_______________________________________________________________________________________\n",
            "D (0.0   ) HAUT     | J (0.0   ) HAUT     | \n",
            "_______________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print_mat_q(game.mat_q, game.space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_XvbUyvnEfa"
      },
      "source": [
        "# Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "id": "SQdRhWhsnEfb"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import tensorflow as tf\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcaCB4TPnEfb"
      },
      "source": [
        "##### Test avec une structure 2 couches denses ayant 16 entrées (nombre de cases) et 4 sorties (4 actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "rXv4ZhrfnEfb"
      },
      "outputs": [],
      "source": [
        "# a class for the game\n",
        "class DeepGame:\n",
        "    # constructor that takes :\n",
        "    # number of episodes : 10000 by default\n",
        "    # number of steps : 100 by default\n",
        "    # is_random_space : False by default\n",
        "    # vec_etat : vector of states (for deep Q learning)\n",
        "    # a model : CNN\n",
        "    # the optimizer name\n",
        "    # the loss function name\n",
        "    # verbose : False by default\n",
        "    def __init__(\n",
        "        self, \n",
        "        episodes = 10000, \n",
        "        steps = 100, \n",
        "        is_random_space = False, \n",
        "        vec_etat = None, \n",
        "        model = None,\n",
        "        optimizer_name = None,\n",
        "        loss_fn_name = None,\n",
        "        verbose = False\n",
        "        ):\n",
        "\n",
        "        self.episodes = episodes\n",
        "        self.steps = steps\n",
        "        self.vec_etat = vec_etat\n",
        "        self.model = model\n",
        "        self.optimizer_name = optimizer_name\n",
        "        self.loss_fn_name = loss_fn_name\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # the space\n",
        "        if is_random_space:\n",
        "            self.space = get_random_space(4, 4, 3)\n",
        "        else:\n",
        "            self.space = get_default_space()\n",
        "\n",
        "        # the vector of states\n",
        "        if vec_etat is None:\n",
        "            self.set_default_vec_etat()\n",
        "\n",
        "        # the model\n",
        "        if model is None:\n",
        "            self.set_default_model()\n",
        "\n",
        "        # the optimizer\n",
        "        self.set_optimizer(optimizer_name)\n",
        "\n",
        "        # the loss function\n",
        "        self.set_loss_fn(loss_fn_name)\n",
        "\n",
        "\n",
        "\n",
        "    # a method to set the vector of states\n",
        "    def set_vec_etat(self, vec_etat):\n",
        "        self.vec_etat = vec_etat\n",
        "\n",
        "    # a method to set the default vector of states\n",
        "    def set_default_vec_etat(self):\n",
        "        self.vec_etat = np.zeros((1, 16))\n",
        "        self.vec_etat[0][0] = 1\n",
        "\n",
        "    # a method to reset the vector of states\n",
        "    def reset_vec_etat(self):\n",
        "        self.vec_etat = np.zeros((1, 16))\n",
        "\n",
        "    # a method to update the vector of states\n",
        "    def update_vec_etat(self, state):\n",
        "        self.reset_vec_etat()\n",
        "        self.vec_etat[0, int(get_lines_size(self.space) * state[0] + state[1])] = 1\n",
        "\n",
        "    # a method to set the model\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    # a method to set the default model\n",
        "    def set_default_model(self):\n",
        "        self.model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(4, activation='relu', input_shape=[16]),\n",
        "            tf.keras.layers.Dense(4, activation='relu'),\n",
        "            tf.keras.layers.Dense(4),\n",
        "        ])\n",
        "\n",
        "    # a method to save the model\n",
        "    def save_model(self):\n",
        "\n",
        "        # number of epochs\n",
        "        E = \"E_\" + str(self.episodes)\n",
        "        # number of steps\n",
        "        S = \"_S_\" + str(self.steps)\n",
        "        # optimizer\n",
        "        O = \"_O_\" + str(self.optimizer_name)\n",
        "        # loss function\n",
        "        L = \"_L_\" + str(self.loss_fn_name)\n",
        "        # dragon reward\n",
        "        DR = \"_DR_\" + str(Rewards.rewards[\"D\"])\n",
        "        # empty reward\n",
        "        ER = \"_ER_\" + str(Rewards.rewards[\"_\"])\n",
        "        # jail reward\n",
        "        JR = \"_JR_\" + str(Rewards.rewards[\"J\"])\n",
        "\n",
        "        # file path\n",
        "        path = \"./seved_models/\" + E + S + O + L + DR + ER + JR + \".h5\"\n",
        "\n",
        "        self.model.save(path)\n",
        "\n",
        "    # a method to set the optimizer\n",
        "    # optimizer : Nadam by default, SGD,Adam\n",
        "    def set_optimizer(self, optimizer):\n",
        "        # set the optimizer according to the name \n",
        "        if optimizer == \"Adam\":\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        elif optimizer == \"SGD\":\n",
        "            self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "        else:\n",
        "            self.optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
        "\n",
        "    # a method to set the loss function\n",
        "    # loss_fn : MSE by default, MAE\n",
        "    def set_loss_fn(self, loss_fn):\n",
        "        # set the loss function according to the name\n",
        "        if loss_fn == \"MAE\":\n",
        "            self.loss_fn = tf.keras.losses.MAE\n",
        "        else:\n",
        "            self.loss_fn = tf.keras.losses.MSE\n",
        "\n",
        "    # function to print if verbose\n",
        "    def print(self, *args):\n",
        "        if self.verbose:\n",
        "            print(*args)\n",
        "\n",
        "    # a method to show progress\n",
        "    def show_progress(self, episode, step, is_random, action, current_case, reward, next_Q_max, target):\n",
        "\n",
        "        if is_random:\n",
        "            choise = \"  (random)  \"\n",
        "        else:\n",
        "            choise = \"  (predict) \"\n",
        "\n",
        "        self.print(\n",
        "            \"episode : \" + str(episode).ljust(5) +\n",
        "            \"| step : \" + str(step).ljust(5) +\n",
        "            \"| action : \" + str(action).ljust(7) + choise +\n",
        "            \"| current_case : \" + str(current_case).ljust(3) +\n",
        "            \"| reward : \" + str(reward).ljust(5) +\n",
        "            \"| next_Q_max : \" + str(next_Q_max).ljust(15) +\n",
        "            \"| target : \" + str(target).ljust(15)\n",
        "        )\n",
        "        \n",
        "\n",
        "    #####################################################################################################################\n",
        "\n",
        "\n",
        "    # a method to choose an action with the epsilon greedy policy\n",
        "    def choose_action(self, state, epsilon):\n",
        "        is_random = random.random() < epsilon\n",
        "        if is_random:\n",
        "            action = Directions.get_random_direction()\n",
        "\n",
        "        else:\n",
        "            Sortie_Q = self.model(self.vec_etat)  # En entrée le vecteur symbolisant l'état\n",
        "            action = Directions.directions[np.argmax(Sortie_Q)] #On sélectionne l'action associée avec la sortie max\n",
        "        return action, is_random\n",
        "     \n",
        "    # Train the model\n",
        "    def train(self):\n",
        "        # create a stable model\n",
        "        model_stable = tf.keras.models.clone_model(self.model)\n",
        "        model_stable.set_weights(self.model.get_weights())\n",
        "\n",
        "        history = np.zeros(self.episodes)\n",
        "\n",
        "        for episode in range(self.episodes):\n",
        "            # reset the position\n",
        "            position = (0, 0)\n",
        "            # calculate the epsilon\n",
        "            epsilon = self.episodes / (self.episodes + episode)\n",
        "\n",
        "            for step in range(self.steps):\n",
        "                # play one step\n",
        "                # choose an action\n",
        "                action, is_random = self.choose_action(position, epsilon)\n",
        "                # apply the action\n",
        "                new_position, reward, fin = applicaion_action(action, position, self.space, self.verbose)\n",
        "  \n",
        "                if fin:\n",
        "                    break\n",
        "\n",
        "                # set weights of the stable model\n",
        "                if step % 10 == 0:\n",
        "                    model_stable.set_weights(self.model.get_weights())\n",
        "\n",
        "                vec_etat_next = np.zeros((1, 16)) # ca sera l'entree du reseau\n",
        "                vec_etat_next[0, int(get_lines_size(self.space) * new_position[0] + new_position[1])] = 1\n",
        "\n",
        "                # model stable predict\n",
        "                next_Q = model_stable.predict(vec_etat_next, verbose=0)\n",
        "                next_Q_max = np.max(next_Q)\n",
        "\n",
        "                # target\n",
        "                target = reward + GAMMA * next_Q_max * (1 - fin)\n",
        "                self.show_progress(episode, step, is_random, action, self.space[position[0]][position[1]], reward, next_Q_max, target)\n",
        "\n",
        "                # gradient descent\n",
        "                with tf.GradientTape() as tape:\n",
        "                    predict = self.model(self.vec_etat) # ce que l'on pense obtenir\n",
        "                    # get index of the action\n",
        "                    action_index = Directions.get_index(action)\n",
        "                    mask = tf.one_hot(action_index, Directions.get_size())\n",
        "                    val_predict = tf.reduce_sum(predict * mask, axis=1)\n",
        "                    loss = self.loss_fn(target, val_predict)\n",
        "\n",
        "                gradients = tape.gradient(loss, self.model.trainable_variables) #calcul du gradient de la focntion loss en fonction des variables du modèle \n",
        "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) # optimisation des paramètres du modèle\n",
        "                history[episode] = loss.numpy() # on récupère la valeur pour afficher l'évolution de l'erreur\n",
        "\n",
        "                # update the position\n",
        "                position = new_position\n",
        "\n",
        "        plt.plot(history, color=\"red\")\n",
        "        plt.title(\"Evolution de l'erreur\")\n",
        "        plt.show()\n",
        "\n",
        "        # save the model\n",
        "        self.save_model()\n",
        "\n",
        "    # play the game\n",
        "    def play(self):\n",
        "\n",
        "        iter = 0\n",
        "        position = (0, 0)\n",
        "        fin = False\n",
        "\n",
        "        while iter < self.steps and not fin:\n",
        "            iter += 1\n",
        "            # update the vector of states\n",
        "            self.update_vec_etat(position)\n",
        "            # choose an action\n",
        "            action, _ = self.choose_action(position, 0)\n",
        "            # apply the action\n",
        "            new_position, reward, fin = applicaion_action(action, position, self.space)\n",
        "\n",
        "            print( str(position) + \" \" + str(action) + \" \" + str(new_position))\n",
        "            if iter % 10 == 0:\n",
        "                print()\n",
        "            \n",
        "            if fin:\n",
        "                print(\"fin de partie en\", iter, \"coups\")\n",
        "                break\n",
        "            # update the position\n",
        "            position = new_position\n",
        "\n",
        "        if iter == self.steps:\n",
        "            print(\"Trop d'itérations\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "p6GfYriK4m1A"
      },
      "outputs": [],
      "source": [
        "# function to create a game with a configuration train and play\n",
        "def create_game(optimizer_name, loss_fn_name, episodes, steps, dragon_reward, jail_reward, empty_reward, start_reward=-5, verbose=False):\n",
        "    # set the rewards\n",
        "    Rewards.set_rewards({\"S\": start_reward, \"D\": dragon_reward, \"J\": jail_reward, \"_\": empty_reward})\n",
        "    # create the game\n",
        "    game = DeepGame(episodes=episodes, steps=steps, optimizer_name=optimizer_name, loss_fn_name=loss_fn_name, verbose=verbose)\n",
        "\n",
        "    game.train()\n",
        "    game.play()\n",
        "\n",
        "    return game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0TgfG6B6UVS"
      },
      "source": [
        "#### TEST\n",
        "\n",
        "\n",
        "*   ***optimizer*** : Adam\n",
        "*   ***loss function*** : SGD\n",
        "*   ***epochs*** : 1000\n",
        "*   ***steps*** : 100\n",
        "*   ***Reward Dragon*** : -500\n",
        "*   ***Reward Empty*** : -20\n",
        "*   ***Reward Jail*** : 500\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C9ocj0r97kU8",
        "outputId": "5db0e1c3-c0a6-45e8-f194-68067836c9a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 1    | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.13109839     | target : -0.8741455459594727\n",
            "episode : 1    | step : 1    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.13109839     | target : -0.8741455459594727\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 2    | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.16255789     | target : -20.843944430351257\n",
            "episode : 2    | step : 1    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.16255789     | target : -0.8439444303512573\n",
            "episode : 2    | step : 2    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.16255789     | target : -0.8439444303512573\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 4    | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.13056064     | target : -0.8746617889404297\n",
            "episode : 4    | step : 1    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.13056064     | target : -0.8746617889404297\n",
            "episode : 4    | step : 2    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.15857217     | target : -20.8477707195282\n",
            "episode : 4    | step : 3    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : 0.13056064     | target : -5.87466178894043\n",
            "episode : 4    | step : 4    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.13056064     | target : -0.8746617889404297\n",
            "episode : 4    | step : 5    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.13056064     | target : -0.8746617889404297\n",
            "episode : 4    | step : 6    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.13056064     | target : -0.8746617889404297\n",
            "episode : 4    | step : 7    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.13056064     | target : -0.8746617889404297\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 6    | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.11966724     | target : -0.8851194500923156\n",
            "episode : 6    | step : 1    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.11966724     | target : -0.8851194500923156\n",
            "episode : 6    | step : 2    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.11966724     | target : -0.8851194500923156\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 7    | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.11749754     | target : -0.8872023606300354\n",
            "episode : 7    | step : 1    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.14961338     | target : -20.856371154785155\n",
            "episode : 7    | step : 2    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.14961338     | target : -0.8563711547851562\n",
            "episode : 7    | step : 3    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.14961338     | target : -0.8563711547851562\n",
            "episode : 7    | step : 4    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : 0.11749754     | target : -5.887202360630035\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 8    | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.14568321     | target : -20.860144114494325\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 9    | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.14449461     | target : -20.86128517627716\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 10   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.107714705    | target : -0.8965938830375672\n",
            "episode : 10   | step : 1    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.107714705    | target : -0.8965938830375672\n",
            "episode : 10   | step : 2    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.14293468     | target : -20.862782707214354\n",
            "episode : 10   | step : 3    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.14293468     | target : -0.8627827072143555\n",
            "episode : 10   | step : 4    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.14293468     | target : -0.8627827072143555\n",
            "episode : 10   | step : 5    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.14293468     | target : -0.8627827072143555\n",
            "episode : 10   | step : 6    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.14293468     | target : -0.8627827072143555\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 14   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.1335577      | target : -20.871784601211548\n",
            "episode : 14   | step : 1    | action : DROITE   (predict) | current_case : _  | reward : -1   | next_Q_max : 0.1335577      | target : -0.8717846012115479\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 15   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.09045642     | target : -0.913161838054657\n",
            "episode : 15   | step : 1    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.09045642     | target : -0.913161838054657\n",
            "episode : 15   | step : 2    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.09045642     | target : -0.913161838054657\n",
            "episode : 15   | step : 3    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.09045642     | target : -0.913161838054657\n",
            "episode : 15   | step : 4    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.13070418     | target : -20.87452398777008\n",
            "episode : 15   | step : 5    | action : DROITE   (predict) | current_case : _  | reward : -1   | next_Q_max : 0.13070418     | target : -0.8745239877700806\n",
            "episode : 15   | step : 6    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : 0.09045642     | target : -5.913161838054657\n",
            "episode : 15   | step : 7    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.09045642     | target : -0.913161838054657\n",
            "episode : 15   | step : 8    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.09045642     | target : -0.913161838054657\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 16   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.07872301     | target : -0.9244259071350098\n",
            "episode : 16   | step : 1    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.07872301     | target : -0.9244259071350098\n",
            "episode : 16   | step : 2    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.07872301     | target : -0.9244259071350098\n",
            "episode : 16   | step : 3    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.12113762     | target : -20.883707885742187\n",
            "episode : 16   | step : 4    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : 0.07872301     | target : -5.92442590713501\n",
            "episode : 16   | step : 5    | action : DROITE   (predict) | current_case : S  | reward : -21  | next_Q_max : 0.12113762     | target : -20.883707885742187\n",
            "episode : 16   | step : 6    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.12113762     | target : -0.8837078857421875\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 18   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.07097902     | target : -0.9318601393699646\n",
            "episode : 18   | step : 1    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.07097902     | target : -0.9318601393699646\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 19   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.06856211     | target : -0.9341803717613221\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 20   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.06754073     | target : -0.9351609015464782\n",
            "episode : 20   | step : 1    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.111920625    | target : -20.892556200027467\n",
            "episode : 20   | step : 2    | action : DROITE   (predict) | current_case : _  | reward : -1   | next_Q_max : 0.111920625    | target : -0.8925562000274658\n",
            "episode : 20   | step : 3    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.111920625    | target : -0.8925562000274658\n",
            "episode : 20   | step : 4    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.111920625    | target : -0.8925562000274658\n",
            "episode : 20   | step : 5    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.111920625    | target : -0.8925562000274658\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 21   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.06053286     | target : -0.9418884539604186\n",
            "episode : 21   | step : 1    | action : DROITE   (predict) | current_case : S  | reward : -21  | next_Q_max : 0.10620096     | target : -20.898047075271606\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 23   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.0582328      | target : -0.9440965127944946\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 24   | step : 0    | action : DROITE   (predict) | current_case : S  | reward : -21  | next_Q_max : 0.10329579     | target : -20.90083604335785\n",
            "episode : 24   | step : 1    | action : DROITE   (predict) | current_case : _  | reward : -1   | next_Q_max : 0.10329579     | target : -0.9008360433578492\n",
            "episode : 24   | step : 2    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.10329579     | target : -0.9008360433578492\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 26   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : 0.05204548     | target : -0.9500363397598267\n",
            "episode : 26   | step : 1    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.099354886    | target : -20.90461930990219\n",
            "episode : 26   | step : 2    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.099354886    | target : -0.9046193099021912\n",
            "episode : 26   | step : 3    | action : DROITE   (predict) | current_case : _  | reward : -1   | next_Q_max : 0.099354886    | target : -0.9046193099021912\n",
            "episode : 26   | step : 4    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.099354886    | target : -0.9046193099021912\n",
            "episode : 26   | step : 5    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : 0.05204548     | target : -5.950036339759826\n",
            "episode : 26   | step : 6    | action : DROITE   (predict) | current_case : S  | reward : -21  | next_Q_max : 0.099354886    | target : -20.90461930990219\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 27   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.09087649     | target : -20.912758569717408\n",
            "episode : 27   | step : 1    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : 0.04149357     | target : -5.960166174173355\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 28   | step : 0    | action : DROITE   (predict) | current_case : S  | reward : -21  | next_Q_max : 0.087792024    | target : -20.915719656944276\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 29   | step : 0    | action : DROITE   (predict) | current_case : S  | reward : -21  | next_Q_max : 0.0859725      | target : -20.917466397285462\n",
            "episode : 29   | step : 1    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.0859725      | target : -0.9174663972854614\n",
            "episode : 29   | step : 2    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : 0.030995343    | target : -5.970244470834732\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 32   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : 0.011887241    | target : -0.9885882484912872\n",
            "episode : 32   | step : 1    | action : DROITE   (predict) | current_case : S  | reward : -21  | next_Q_max : 0.08015716     | target : -20.923049125671387\n",
            "episode : 32   | step : 2    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.08015716     | target : -0.9230491256713867\n",
            "episode : 32   | step : 3    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.08015716     | target : -0.9230491256713867\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 33   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.07346859     | target : -20.929470155239105\n",
            "episode : 33   | step : 1    | action : DROITE   (predict) | current_case : _  | reward : -1   | next_Q_max : 0.07346859     | target : -0.9294701552391053\n",
            "episode : 33   | step : 2    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.07346859     | target : -0.9294701552391053\n",
            "episode : 33   | step : 3    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.07346859     | target : -0.9294701552391053\n",
            "episode : 33   | step : 4    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.07346859     | target : -0.9294701552391053\n",
            "episode : 33   | step : 5    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.07346859     | target : -0.9294701552391053\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 34   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.044661418   | target : -1.042874960899353\n",
            "episode : 34   | step : 1    | action : DROITE   (predict) | current_case : S  | reward : -21  | next_Q_max : 0.06453745     | target : -20.938044047355653\n",
            "episode : 34   | step : 2    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.06453745     | target : -0.9380440473556518\n",
            "episode : 34   | step : 3    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : -0.044661418   | target : -6.042874960899353\n",
            "episode : 34   | step : 4    | action : DROITE   (predict) | current_case : S  | reward : -21  | next_Q_max : 0.06453745     | target : -20.938044047355653\n",
            "episode : 34   | step : 5    | action : DROITE   (predict) | current_case : _  | reward : -1   | next_Q_max : 0.06453745     | target : -0.9380440473556518\n",
            "episode : 34   | step : 6    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.06453745     | target : -0.9380440473556518\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 35   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.08037819    | target : -1.077163062095642\n",
            "episode : 35   | step : 1    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.08037819    | target : -1.077163062095642\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 38   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.08677719    | target : -1.0833061003684998\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 41   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.08737475    | target : -1.0838797569274903\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 43   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.05155662     | target : -20.950505644083023\n",
            "episode : 43   | step : 1    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.05155662     | target : -0.9505056440830231\n",
            "episode : 43   | step : 2    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.05155662     | target : -0.9505056440830231\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 45   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.08988936    | target : -1.0862937879562378\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 46   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.0904531     | target : -1.086834979057312\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 47   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.046372745    | target : -20.955482164621355\n",
            "episode : 47   | step : 1    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : -0.09098773    | target : -6.0873482179641725\n",
            "episode : 47   | step : 2    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.09098773    | target : -1.0873482179641725\n",
            "episode : 47   | step : 3    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.09098773    | target : -1.0873482179641725\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 48   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.09374456    | target : -1.0899947786331177\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 50   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.0943553     | target : -1.090581088066101\n",
            "episode : 50   | step : 1    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.0943553     | target : -1.090581088066101\n",
            "episode : 50   | step : 2    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.04145426     | target : -20.960203911066056\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 51   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.03909597     | target : -20.962467867136002\n",
            "episode : 51   | step : 1    | action : GAUCHE   (random)  | current_case : _  | reward : -6   | next_Q_max : -0.09612836    | target : -6.092283225059509\n",
            "episode : 51   | step : 2    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.09612836    | target : -1.0922832250595094\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 52   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.035357315    | target : -20.96605697751045\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 53   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.09931509    | target : -1.0953424882888794\n",
            "episode : 53   | step : 1    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.09931509    | target : -1.0953424882888794\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 55   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.100821905   | target : -1.0967890286445618\n",
            "episode : 55   | step : 1    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.031423546    | target : -20.96983339548111\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 58   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.102237135   | target : -1.0981476497650147\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 65   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.102923065   | target : -1.0988061428070068\n",
            "episode : 65   | step : 1    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.027700737    | target : -20.973407292366026\n",
            "episode : 65   | step : 2    | action : DROITE   (random)  | current_case : _  | reward : -1   | next_Q_max : 0.027700737    | target : -0.9734072923660279\n",
            "episode : 65   | step : 3    | action : HAUT     (random)  | current_case : _  | reward : -1   | next_Q_max : 0.027700737    | target : -0.9734072923660279\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 66   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.10550028    | target : -1.1012802696228028\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 71   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.10605802    | target : -1.1018156957626344\n",
            "episode : 71   | step : 1    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.10605802    | target : -1.1018156957626344\n",
            "episode : 71   | step : 2    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.10605802    | target : -1.1018156957626344\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 72   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.10749338    | target : -1.103193643093109\n",
            "episode : 72   | step : 1    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.018815339    | target : -20.98193727493286\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 73   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.017024316    | target : -20.983656656742095\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 79   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.10903034    | target : -1.1046691298484803\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 80   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.109587535   | target : -1.1052040338516236\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 82   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.013165586    | target : -20.98736103773117\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 83   | step : 0    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.110696174   | target : -1.1062683272361755\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 84   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.010362543    | target : -20.990051958560944\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 85   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.11442803    | target : -1.1098509073257445\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 86   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.116365835   | target : -1.1117112016677857\n",
            "episode : 86   | step : 1    | action : HAUT     (random)  | current_case : S  | reward : -1   | next_Q_max : -0.116365835   | target : -1.1117112016677857\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 93   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.0053166      | target : -20.994896063804628\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 95   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.12165988    | target : -1.1167934870719909\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 96   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.12341311    | target : -1.118476583957672\n",
            "episode : 96   | step : 1    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : 0.0029657558   | target : -20.99715287446976\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "episode : 97   | step : 0    | action : GAUCHE   (random)  | current_case : S  | reward : -1   | next_Q_max : -0.12693067    | target : -1.1218534421920776\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n",
            "episode : 98   | step : 0    | action : DROITE   (random)  | current_case : S  | reward : -21  | next_Q_max : -0.000279814   | target : -21.000268621444704\n",
            "\u001b[92m*************************************************************************************\n",
            "*********************************** YOU WIN *****************************************\n",
            "*************************************************************************************\n",
            "\u001b[0m\n",
            "\u001b[91m*********************************** YOU LOOSE ****************************************\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0) BAS (0, 0)\n",
            "fin de partie en 1 coups\n"
          ]
        }
      ],
      "source": [
        "game1 = create_game(optimizer_name=\"Adam\", loss_fn_name=\"SGD\", episodes=100, steps=100, dragon_reward=-500, empty_reward=-20, jail_reward=500, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5uSY-7_7agv"
      },
      "source": [
        "#### TEST\n",
        "\n",
        "\n",
        "*   ***optimizer*** : Nadam\n",
        "*   ***loss function*** : MSE\n",
        "*   ***epochs*** : 5000\n",
        "*   ***steps*** : 100\n",
        "*   ***Reward Dragon*** : -100\n",
        "*   ***Reward Empty*** : -10\n",
        "*   ***Reward Jail*** : 1000\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "f_LPDJcA7c11"
      },
      "outputs": [],
      "source": [
        "#game2 = create_game(optimizer_name=\"Nadam\", loss_fn_name=\"MSE\", episodes=5000, steps=100, dragon_reward=-100, empty_reward=-10, jail_reward=1000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4428cbe1ba9314b3551257500664b995dcc328d303584ff4cad6f1a703111ed9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}