{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5HGSGxhnEfS"
      },
      "source": [
        "# 1. Développement d'un jeu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "16u0v0mvnEfU"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6bQNPZzonEfV"
      },
      "outputs": [],
      "source": [
        "# configuration\n",
        "GAMMA = 0.96\n",
        "ALPHA = 0.81"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehrDGUS-pwUG"
      },
      "source": [
        "##### Créer une fonction qui permet de simuler le plateau en positionnant notamment les différents éléments (case départ, fin, dragons)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dWZoG4tanEfW"
      },
      "outputs": [],
      "source": [
        "#for the space\n",
        "\"\"\"\n",
        "by default the default space is used\n",
        "the space is a 2D array 4 * 4 of characters\n",
        "the characters are: \n",
        "    'S' : the starting point\n",
        "    '_' : empty space\n",
        "    'J' : the goal\n",
        "    'D' : a dragon\n",
        "the default space is:\n",
        "    S___\n",
        "    D_D_ \n",
        "    ___D\n",
        "    _D_J\n",
        "\"\"\"\n",
        "\n",
        "# the default space\n",
        "default_space = [\n",
        "        ['S', '_', '_', '_'],\n",
        "        ['D', '_', 'D', '_'],\n",
        "        ['_', '_', '_', 'D'],\n",
        "        ['_', 'D', '_', 'J']\n",
        "    ]\n",
        "\n",
        "def get_default_space():\n",
        "    return default_space\n",
        "\n",
        "# a random space with number of lines and columns and a number of dragons\n",
        "def get_random_space(lines, columns, dragons):\n",
        "    space = []\n",
        "    for l in range(lines):\n",
        "        space.append([])\n",
        "        for c in range(columns):\n",
        "            space[l].append('_')\n",
        "\n",
        "    space[0][0] = 'S'\n",
        "    space[lines-1][columns-1] = 'J'\n",
        "    \n",
        "    i = 0\n",
        "    while i < dragons:\n",
        "        l = random.randint(0, lines-1)\n",
        "        c = random.randint(0, columns-1)\n",
        "        if space[l][c] == '_':\n",
        "            space[l][c] = 'D'\n",
        "            i += 1\n",
        "        else:\n",
        "            i -= 1\n",
        "\n",
        "    return space\n",
        "\n",
        "# pretty print the space\n",
        "def print_space(space):\n",
        "    for l in space:\n",
        "        for c in l:\n",
        "            print(c, end='| ')\n",
        "        \n",
        "        print()\n",
        "\n",
        "# get the size of the lines\n",
        "def get_lines_size(space):\n",
        "    return len(space)\n",
        "\n",
        "# get the size of the columns\n",
        "def get_columns_size(space):\n",
        "    return len(space[0])\n",
        "\n",
        "# get the size of the space\n",
        "def get_size(space):\n",
        "    return get_lines_size(space) * get_columns_size(space)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-eVBWhkOnEfX"
      },
      "outputs": [],
      "source": [
        "# a static class for the rewards\n",
        "class Rewards:\n",
        "    # the rewards for each character\n",
        "    rewards = {\n",
        "        'S': 0,\n",
        "        '_': 0,\n",
        "        'J': 1,\n",
        "        'D': -1\n",
        "    }\n",
        "\n",
        "    # a method to get the reward of a character\n",
        "    def get_reward(character):\n",
        "        return Rewards.rewards.get(character)\n",
        "\n",
        "    # a method to set the reward \n",
        "    def set_rewards(rewards):\n",
        "        Rewards.rewards = rewards\n",
        "\n",
        "    # a method to set the reward of a character\n",
        "    def set_reward(character, reward):\n",
        "        Rewards.rewards[character] = reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qL0SIBRpwUI"
      },
      "source": [
        "##### Créer une fonction qui permet de simuler l'interaction entre l'agent et son environnement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "amIQUyAKnEfX"
      },
      "outputs": [],
      "source": [
        "# a static class for the Directions\n",
        "class Directions:\n",
        "    # the directions\n",
        "    directions = [\"HAUT\", \"DROITE\", \"BAS\", \"GAUCHE\"]\n",
        "\n",
        "    # a method to get the size of the directions\n",
        "    def get_size():\n",
        "        return len(Directions.directions)\n",
        "\n",
        "    # a method to get the index of a direction\n",
        "    def get_index(direction):\n",
        "        return Directions.directions.index(direction)\n",
        "\n",
        "    # a method to get a random direction\n",
        "    def get_random_direction():\n",
        "        return random.choice(Directions.directions)\n",
        "\n",
        "    # a method to get the direction that maximizes the Q value\n",
        "    def get_max_direction(mat_q, state):\n",
        "        return Directions.directions[np.argmax(mat_q[state])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "emfLzRnwnEfY"
      },
      "outputs": [],
      "source": [
        "def isFin (space, position, verbose = False):\n",
        "    (l,c) = position\n",
        "\n",
        "    # if the player is at the goal, back to the starting point\n",
        "    if(space[l][c]== 'J'):\n",
        "        # print in green win\n",
        "        if verbose:\n",
        "            print(\"\\033[92m\" + \n",
        "            \"***********************************************************************************************************************************************************\\n\" +\n",
        "            \"********************************************************************** YOU WIN ****************************************************************************\\n\" +\n",
        "            \"***********************************************************************************************************************************************************\\n\" +\n",
        "            \"\\033[0m\")\n",
        "        return True, (0,0)\n",
        "\n",
        "    # # if the player ecnounters a dragon, back to the starting point\n",
        "    if(space[l][c]== 'D'):\n",
        "        # print in red loose\n",
        "        if verbose:\n",
        "            print(\"\\033[91m\" +\n",
        "            \"********************************************************************** YOU LOOSE ***************************************************************************\\n\" +\n",
        "            \"\\033[0m\")\n",
        "        return True, (0,0)\n",
        "\n",
        "    return False, position\n",
        "\n",
        "# a method to apply an action to the player\n",
        "def applicaion_action(action, position, space, verbose = False):\n",
        "\n",
        "    (l, c) = position\n",
        "    nextPos = position\n",
        "\n",
        "    if action == \"HAUT\":\n",
        "        nextPos = (l-1,c)\n",
        "    elif action == \"DROITE\":\n",
        "        nextPos = (l,c+1)\n",
        "    elif action == \"BAS\":\n",
        "        nextPos = (l+1,c);\n",
        "    elif action == \"GAUCHE\":\n",
        "        nextPos = (l,c-1);\n",
        "\n",
        "    # check if the next position is in the space\n",
        "    if (nextPos[0] < len(space) and nextPos[1] < len(space) and nextPos[0] >=0 and nextPos[1] >=0 ):\n",
        "        position = nextPos\n",
        "        \n",
        "    # get the current case in the space\n",
        "    case = space[position[0]][position[1]]\n",
        "    # set the reward\n",
        "    reward = Rewards.get_reward(case)\n",
        "\n",
        "    # check if the player is at the goal\n",
        "    fin, position = isFin(space, position, verbose)\n",
        "\n",
        "    return [position, reward, fin]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3CB8YzfpwUK"
      },
      "source": [
        "### Donner quelques éléments de commentaire sur la mise en place de l'environnement de jeux\n",
        "\n",
        "##### 1) Le plateau\n",
        "\n",
        "Pour créer un environnement de jeux, nous avons d'abord créé une la variable ***default_space*** qui est un tableau 2 dimensions **(4*4)** de charactères qui representent les cases du plateau de jeu proposé. Les cases sont représentées par les caractères suivants:\n",
        "\n",
        "*   **\"_\"** représente une case vide\n",
        "*   **\"D\"** représente une case avec un dragon\n",
        "*   **\"S\"** représente la case de départ\n",
        "*   **\"J\"** représente la case d'arrivée (JAIL)\n",
        "\n",
        "\n",
        "Nous avons aussi créé une fonction ***get_random_space*** qui permet de générer un plateau de jeu aléatoire en precisant le nombre de lignes, de colonnes etle nombre de dragons à placer sur le plateau.\n",
        "\n",
        "Nous avons aussi créé une fonction ***print_space*** qui permet d'afficher le plateau de jeu.\n",
        "\n",
        "##### 2) L'interaction\n",
        "\n",
        "Avec la function ***application_action*** nous créons une nouvelle position ***nextPos*** en fonction de l'***action***, et nous verifions si la nouvelle position est valide ou non. \n",
        "\n",
        "Si la nouvelle position est valide, nous mettons à jour la position de l'agent et nous recupérons la récompense associée à cette nouvelle position.\n",
        "\n",
        "Si la nouvelle position n'est pas valide, nous ne mettons pas à jour la position de l'agent et nous recupérons la récompense associée à la position actuelle.\n",
        "\n",
        "Puis nous verifions si la partie est terminée ou non (si l'agent est arrivé à la case d'arrivée ou s'il tombé sur un dragon), si la partie est terminée, nous réinitialisons la position de l'agent à la case de départ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDxUSIRynEfZ"
      },
      "source": [
        "# 2. Développement du Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ug3WW4dUpwUL"
      },
      "outputs": [],
      "source": [
        "# for the Q matrix\n",
        "# init the Q matrix with zeros and the size of the space and the directions length\n",
        "def init_mat_q(space):\n",
        "    return np.zeros((get_lines_size(space), get_columns_size(space), Directions.get_size()))\n",
        "\n",
        "# get the Q value of a state and a direction\n",
        "def get_q_value(mat_q, state, direction):\n",
        "    return mat_q[state][Directions.get_index(direction)]\n",
        "\n",
        "# update the Q matrix\n",
        "# according to state, action, reward, next_state, ALPHA and GAMMA\n",
        "def update_mat_q(mat_q, state, action, reward, next_state):\n",
        "    mat_q[state][Directions.get_index(action)] += ALPHA * (reward + GAMMA * np.max(mat_q[next_state]) - mat_q[state][Directions.get_index(action)])\n",
        "    return mat_q\n",
        "\n",
        "# pretty print the space\n",
        "def print_mat_q(mat_q, space):\n",
        "    def get_best_direction(l, c):\n",
        "        return Directions.directions[np.argmax(mat_q[l][c])]\n",
        "\n",
        "    for l in range(get_lines_size(space)):\n",
        "        for c in range(get_columns_size(space)):\n",
        "            case      = space[l][c]\n",
        "            direction = get_best_direction(l, c)\n",
        "            q_value   = str(round(get_q_value(mat_q, (l, c), direction), 2)).ljust(6)\n",
        "\n",
        "            content = case + \" (\" + q_value + \") \" + direction\n",
        "            print(content.ljust(20), end='| ')\n",
        "        print(\"\\n_______________________________________________________________________________________\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "TIz5A3zznEfZ"
      },
      "outputs": [],
      "source": [
        "# a class for the game\n",
        "class Game:\n",
        "    # constructor that takes :\n",
        "    # number of episodes : 10000 by default\n",
        "    # number of steps : 100 by default\n",
        "    # is_random_space : False by default\n",
        "    # a Q matrix : initialized with zeros (with the size of the space and the number of directions)\n",
        "    def __init__(self, episodes = 10000, steps = 100, is_random_space = False, mat_q = None):\n",
        "        self.episodes = episodes\n",
        "        self.steps = steps\n",
        "        # the space\n",
        "        if is_random_space:\n",
        "            self.space = get_random_space(4, 4, 3)\n",
        "        else:\n",
        "            self.space = get_default_space()\n",
        "        \n",
        "        # the Q matrix \n",
        "        if mat_q is None:\n",
        "            self.mat_q = init_mat_q(self.space)\n",
        "\n",
        "    # a method to choose an action with the epsilon greedy policy\n",
        "    def choose_action(self, state, epsilon, mat_q):\n",
        "        if random.random() < epsilon:\n",
        "            return Directions.get_random_direction()\n",
        "        else:\n",
        "            return Directions.get_max_direction(mat_q, state)\n",
        "\n",
        "    # a method to play one step (with mat_q, state, epsilon)\n",
        "    def oneStep(self, mat_q, state, epsilon, verbose):\n",
        "        # choose an action\n",
        "        action = self.choose_action(state, epsilon, mat_q)\n",
        "        if verbose:\n",
        "            print(action, end=', ')\n",
        "        # apply the action\n",
        "        new_state, reward, fin = applicaion_action(action, state, self.space)\n",
        "        # update the Q matrix\n",
        "        new_q = update_mat_q(mat_q, state, action, reward, new_state)\n",
        "        return new_q, new_state, fin\n",
        "\n",
        "    # a method to apply the algorithm \n",
        "    def apply_algorithm(self):\n",
        "        total_steps = 0\n",
        "        # apply the algorithm \n",
        "        for episode in range(self.episodes):\n",
        "            # reset the position\n",
        "            position = (0, 0)\n",
        "            # calculate the epsilon\n",
        "            epsilon = self.episodes / (self.episodes + episode)\n",
        "            #print(\"epsilon : \", epsilon)\n",
        "\n",
        "            # play the game\n",
        "            for step in range(1, self.steps):\n",
        "                # play one step\n",
        "                self.mat_q, position, fin = self.oneStep(self.mat_q, position, epsilon, False)\n",
        "                # if the game is finished\n",
        "                if fin:\n",
        "                    total_steps += step\n",
        "                    break\n",
        "\n",
        "        print(\"total steps : \", total_steps)\n",
        "        print(\"average steps : \", total_steps / self.episodes)\n",
        "\n",
        "    # a method to play the game\n",
        "    def play(self):\n",
        "        # reset the position\n",
        "        position = (0, 0)\n",
        "        # play the game\n",
        "        for step in range(1, self.steps):\n",
        "            # play one step\n",
        "            self.mat_q, position, fin = self.oneStep(self.mat_q, position, 0, True)\n",
        "            # if the game is finished\n",
        "            if fin:\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWZfAABqnEfa",
        "outputId": "ebf9ad2d-fd0a-4b43-c17e-e7ba0f7631c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total steps :  58463\n",
            "average steps :  5.8463\n"
          ]
        }
      ],
      "source": [
        "# PLAY\n",
        "game = Game()\n",
        "game.apply_algorithm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncr7KZt_jiZ1"
      },
      "source": [
        "### Donner des éléments de commentaire sur la stratégie que vous avez utilisée pour développer l'algorithme de Q-Learning.\n",
        "\n",
        "##### 1) La matrice Q\n",
        "\n",
        "Nous avons :\n",
        "*   Une function ***init_mat_q*** qui prend en paramètre le plateau de jeu et elle retourne la matrice Q ***(nombre de ligne * nombre de ligne * Nombre de directions)*** initialisée à 0.\n",
        "*   Une function ***update_mat_q*** qui prend en paramètre la matrice Q, la position actuelle, la position suivante, l'action, la récompense et met à jour la matrice Q à la position actuelle et et pour la direction de l'action en utilisant la formule du cours.\n",
        "\n",
        "\n",
        "##### 2) L'algorithme Q-learning\n",
        "\n",
        "Nous avons une class ***Game***, son constructeur prend en paramètre :\n",
        "*   Le nombre de parties à jouer (10000 par défaut)\n",
        "*   Le nombre de coups par partie (100 par défaut)\n",
        "*   Un booléen ***is_random_space*** (False par défaut) qui permet de choisir si on veut un plateau de jeu aléatoire ou celui proposé par défaut\n",
        "*   Une matrice Q (None par défaut) qui permet de choisir si on veut utiliser une matrice Q déjà existante ou une nouvelle matrice Q qu'on va générer à partir du plateau de jeu\n",
        "\n",
        "La stratégie utilisée est l'***epsilon-greedy***, on choisit une action aléatoire avec une probabilité ***epsilon***, sinon on choisit l'action qui maximise la récompense.\n",
        "\n",
        "Dans la boucle pricipale de l'algorithme, l'epsilon est une fraction (nombre de parties  à jouer / nombre de parties  à jouer + nombre de parties jouées), donc au fur et à mesure que le nombre de parties jouées augmente, l'epsilon diminue.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0yLMr9AjiZ2",
        "outputId": "d5728279-7196-445d-8fd0-98e33e1f8105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[3.603157   3.75328854 2.603157   3.603157  ]\n",
            "  [3.75328854 3.603157   3.90967556 3.603157  ]\n",
            "  [3.603157   3.45903072 2.603157   3.75328854]\n",
            "  [3.45903072 3.45903072 3.32066949 3.603157  ]]\n",
            "\n",
            " [[0.         0.         0.         0.        ]\n",
            "  [3.75328854 2.603157   4.07257871 2.603157  ]\n",
            "  [0.         0.         0.         0.        ]\n",
            "  [3.45903072 3.32066949 2.603157   2.60315684]]\n",
            "\n",
            " [[2.603157   4.07257871 3.75328854 3.90967556]\n",
            "  [3.90967556 4.24226949 2.603157   3.90967556]\n",
            "  [2.603157   2.603157   4.41903072 4.07257871]\n",
            "  [0.         0.         0.         0.        ]]\n",
            "\n",
            " [[3.90967556 2.603157   3.75328854 3.75328854]\n",
            "  [0.         0.         0.         0.        ]\n",
            "  [4.24226949 4.603157   4.41903072 2.603157  ]\n",
            "  [0.         0.         0.         0.        ]]]\n"
          ]
        }
      ],
      "source": [
        "print(game.mat_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6AXQlAnnEfa",
        "outputId": "0d0889a4-108e-42f2-d64f-1caff889d6c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DROITE, BAS, BAS, DROITE, BAS, DROITE, fin de partie en 6 coups\n"
          ]
        }
      ],
      "source": [
        "# play a with the optimal policy\n",
        "position = (0, 0)\n",
        "for step in range(1, game.steps):\n",
        "    # play one step\n",
        "    game.mat_q, position, fin = game.oneStep(game.mat_q, position, 0, True)\n",
        "    if fin:\n",
        "        print(\"fin de partie en\", step, \"coups\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rYr5rkcnEfa",
        "outputId": "ced52dde-4305-445b-8c68-79e1dc140bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S (3.75  ) DROITE   | _ (3.91  ) BAS      | _ (3.75  ) GAUCHE   | _ (3.6   ) GAUCHE   | \n",
            "_______________________________________________________________________________________\n",
            "D (0.0   ) HAUT     | _ (4.07  ) BAS      | D (0.0   ) HAUT     | _ (3.46  ) HAUT     | \n",
            "_______________________________________________________________________________________\n",
            "_ (4.07  ) DROITE   | _ (4.24  ) DROITE   | _ (4.42  ) BAS      | D (0.0   ) HAUT     | \n",
            "_______________________________________________________________________________________\n",
            "_ (3.91  ) HAUT     | D (0.0   ) HAUT     | _ (4.6   ) DROITE   | J (0.0   ) HAUT     | \n",
            "_______________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print_mat_q(game.mat_q, game.space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vs_2S4EjiZ3"
      },
      "source": [
        "#### Analyser la table obtenue, montrer par simulation que votre politique ainsi définie fonctionne\n",
        "\n",
        "Dans la table obtenue, chaque ligne represente une case du plateau, une ligne est de la forme [recompense en allant en ***HAUT***, recompense en allant à ***GAUCHE***, recompense en allant en ***BAS***,  recompense en allant à ***DROITE***].\n",
        "\n",
        "Nous pouvons voir que dans chaque ligne, la recompense maximale est associée à la direction qui permet de gagner la partie. Ce qui signifie que notre politique est correcte.\n",
        "\n",
        "Notre simulation montre que notre politique fonctionne, car l'agent arrive à la case d'arrivée en 6 coups, ce qui est le nombre de coups minimum pour gagner la partie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIwfF1StjiZ3"
      },
      "source": [
        "### Test avec différents paramètres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PErqn7f2jiZ4"
      },
      "outputs": [],
      "source": [
        "def play_with_rewards(start_reward, dragon_reward, empty_reward, jail_reward):\n",
        "    # set the rewards\n",
        "    Rewards.set_rewards({\"S\": start_reward, \"D\": dragon_reward, \"J\": jail_reward, \"_\": empty_reward})\n",
        "\n",
        "    # play\n",
        "    game = Game()\n",
        "    game.apply_algorithm()\n",
        "    game.play()\n",
        "    print_mat_q(game.mat_q, game.space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "#play_with_rewards(0, -1, -0.1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "#play_with_rewards(0, -1, -0.1, -10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "#play_with_rewards(0, -1, -0.1, -100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "#play_with_rewards(0, -1, -0.1, -100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_XvbUyvnEfa"
      },
      "source": [
        "# Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "SQdRhWhsnEfb"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import tensorflow as tf\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "TO9tRtzvjiZ4"
      },
      "outputs": [],
      "source": [
        "# configuration\n",
        "GAMMA = 0.999\n",
        "ALPHA = 0.81"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcaCB4TPnEfb"
      },
      "source": [
        "##### Test avec une structure 2 couches denses ayant 16 entrées (nombre de cases) et 4 sorties (4 actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "rXv4ZhrfnEfb"
      },
      "outputs": [],
      "source": [
        "# a class for the game\n",
        "class DeepGame:\n",
        "    # constructor that takes :\n",
        "    # number of episodes : 10000 by default\n",
        "    # number of steps : 100 by default\n",
        "    # is_random_space : False by default\n",
        "    # vec_etat : vector of states (for deep Q learning)\n",
        "    # a model : CNN\n",
        "    # the optimizer name\n",
        "    # the loss function name\n",
        "    # verbose : False by default\n",
        "    def __init__(\n",
        "        self, \n",
        "        episodes = 10000, \n",
        "        steps = 100, \n",
        "        is_random_space = False, \n",
        "        vec_etat = None, \n",
        "        model = None,\n",
        "        optimizer_name = \"Nadam\",\n",
        "        loss_fn_name = None,\n",
        "        verbose = False\n",
        "        ):\n",
        "\n",
        "        self.episodes = episodes\n",
        "        self.steps = steps\n",
        "        self.vec_etat = vec_etat\n",
        "        self.model = model\n",
        "        self.optimizer_name = optimizer_name\n",
        "        self.loss_fn_name = loss_fn_name\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # the space\n",
        "        if is_random_space:\n",
        "            self.space = get_random_space(4, 4, 3)\n",
        "        else:\n",
        "            self.space = get_default_space()\n",
        "\n",
        "        # the vector of states\n",
        "        if vec_etat is None:\n",
        "            self.set_default_vec_etat()\n",
        "\n",
        "        # the model\n",
        "        if model is None:\n",
        "            self.set_default_model()\n",
        "\n",
        "        # the optimizer\n",
        "        self.set_optimizer(optimizer_name)\n",
        "\n",
        "        # the loss function\n",
        "        self.set_loss_fn(loss_fn_name)\n",
        "\n",
        "\n",
        "\n",
        "    # a method to set the vector of states\n",
        "    def set_vec_etat(self, vec_etat):\n",
        "        self.vec_etat = vec_etat\n",
        "\n",
        "    # a method to set the default vector of states\n",
        "    def set_default_vec_etat(self):\n",
        "        self.vec_etat = np.zeros((1, get_size(self.space)))\n",
        "        self.vec_etat[0][0] = 1\n",
        "\n",
        "    # a method to reset the vector of states\n",
        "    def reset_vec_etat(self):\n",
        "        self.vec_etat = np.zeros((1, get_size(self.space)))\n",
        "\n",
        "    # a method to update the vector of states\n",
        "    def update_vec_etat(self, state):\n",
        "        self.reset_vec_etat()\n",
        "        self.vec_etat[0, int(get_lines_size(self.space) * state[0] + state[1])] = 1\n",
        "\n",
        "    # a method to set the model\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    # a method to set the default model\n",
        "    def set_default_model(self):\n",
        "        self.model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Dense(4, activation='relu', input_shape=[get_size(self.space)]),\n",
        "            tf.keras.layers.Dense(4, activation='relu'),\n",
        "            tf.keras.layers.Dense(4),\n",
        "        ])\n",
        "\n",
        "    # a method to save the model\n",
        "    def save_model(self):\n",
        "\n",
        "        # number of epochs\n",
        "        E = \"E_\" + str(self.episodes)\n",
        "        # number of steps\n",
        "        S = \"_S_\" + str(self.steps)\n",
        "        # optimizer\n",
        "        O = \"_O_\" + str(self.optimizer_name)\n",
        "        # loss function\n",
        "        L = \"_L_\" + str(self.loss_fn_name)\n",
        "        # dragon reward\n",
        "        DR = \"_DR_\" + str(Rewards.rewards[\"D\"])\n",
        "        # empty reward\n",
        "        ER = \"_ER_\" + str(Rewards.rewards[\"_\"])\n",
        "        # jail reward\n",
        "        JR = \"_JR_\" + str(Rewards.rewards[\"J\"])\n",
        "\n",
        "        # file path\n",
        "        path = \"./seved_models/\" + E + S + O + L + DR + ER + JR + \".h5\"\n",
        "\n",
        "        self.model.save(path)\n",
        "\n",
        "    # a method to set the optimizer\n",
        "    # optimizer_name : Nadam by default, SGD,Adam\n",
        "    def set_optimizer(self, optimizer_name):\n",
        "        # set the optimizer according to the name \n",
        "        if optimizer_name == \"Adam\":\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
        "        elif optimizer_name == \"SGD\":\n",
        "            self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.00001)\n",
        "        else:\n",
        "            self.optimizer = tf.keras.optimizers.Nadam(learning_rate=0.00001)\n",
        "\n",
        "    # a method to set the loss function\n",
        "    # loss_fn : MSE by default, MAE\n",
        "    def set_loss_fn(self, loss_fn):\n",
        "        # set the loss function according to the name\n",
        "        if loss_fn == \"MAE\":\n",
        "            self.loss_fn = tf.keras.losses.MAE\n",
        "        else:\n",
        "            self.loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "    # function to print if verbose\n",
        "    def print(self, *args):\n",
        "        if self.verbose:\n",
        "            print(*args)\n",
        "\n",
        "    # a method to show progress\n",
        "    def show_progress(self, episode, step, is_random, action, current_case, reward, next_Q_max, target):\n",
        "        choice = \"  (random)  \" if is_random else \"  (predict) \"\n",
        "\n",
        "        self.print(\n",
        "            \"episode : \" + str(episode).ljust(5) +\n",
        "            \"| step : \" + str(step).ljust(5) +\n",
        "            \"| action : \" + str(action).ljust(7) + choice +\n",
        "            \"| current_case : \" + str(current_case).ljust(3) +\n",
        "            \"| reward : \" + str(reward).ljust(5) +\n",
        "            \"| next_Q_max : \" + str(next_Q_max).ljust(15) +\n",
        "            \"| target : \" + str(target).ljust(15)\n",
        "        )\n",
        "        \n",
        "\n",
        "    #####################################################################################################################\n",
        "\n",
        "\n",
        "    # a method to choose an action with the epsilon greedy policy\n",
        "    def choose_action(self, state, epsilon):\n",
        "        is_random = np.random.uniform() < epsilon\n",
        "        if is_random:\n",
        "            action = Directions.get_random_direction()\n",
        "        else:\n",
        "            Sortie_Q = self.model(self.vec_etat)  # En entrée le vecteur symbolisant l'état\n",
        "            action = Directions.directions[np.argmax(Sortie_Q)] #On sélectionne l'action associée avec la sortie max\n",
        "        return action, is_random\n",
        "     \n",
        "    # Train the model\n",
        "    def train(self):\n",
        "        # create a stable model\n",
        "        model_stable = tf.keras.models.clone_model(self.model)\n",
        "        model_stable.set_weights(self.model.get_weights())\n",
        "\n",
        "        loss_history = np.zeros(self.episodes)\n",
        "        reward_history = np.zeros(self.episodes)\n",
        "\n",
        "        for episode in range(self.episodes):\n",
        "            # reset the position\n",
        "            position = (0, 0)\n",
        "            # calculate the epsilon\n",
        "            epsilon = self.episodes / (self.episodes + episode)\n",
        "            loss = 0\n",
        "\n",
        "            for step in range(self.steps):\n",
        "                # play one step\n",
        "                # choose an action\n",
        "                action, is_random = self.choose_action(position, epsilon)\n",
        "                # apply the action\n",
        "                position, reward, fin = applicaion_action(action, position, self.space, self.verbose)\n",
        "                # add the reward\n",
        "                reward_history[episode] += reward\n",
        "  \n",
        "                if fin:\n",
        "                    break\n",
        "\n",
        "                # set weights of the stable model\n",
        "                if step % 10 == 0:\n",
        "                    model_stable.set_weights(self.model.get_weights())\n",
        "\n",
        "                vec_etat_next = np.zeros((1, get_size(self.space))) # ca sera l'entree du reseau\n",
        "                vec_etat_next[0, int(get_lines_size(self.space) * position[0] + position[1])] = 1\n",
        "\n",
        "                # model stable predict\n",
        "                next_Q = model_stable.predict(vec_etat_next, verbose=0)\n",
        "                next_Q_max = np.max(next_Q)\n",
        "\n",
        "                # target\n",
        "                target = reward + GAMMA * next_Q_max * (1 - fin)\n",
        "                self.show_progress(episode, step, is_random, action, self.space[position[0]][position[1]], reward, next_Q_max, target)\n",
        "\n",
        "                # gradient descent\n",
        "                with tf.GradientTape() as tape:\n",
        "                    predict = self.model(self.vec_etat) # ce que l'on pense obtenir\n",
        "                    # get index of the action\n",
        "                    action_index = Directions.get_index(action)\n",
        "                    mask = tf.one_hot(action_index, Directions.get_size())\n",
        "                    val_predict = tf.reduce_sum(predict * mask, axis=1)\n",
        "                    loss += self.loss_fn(target, val_predict)\n",
        "\n",
        "                gradients = tape.gradient(loss, self.model.trainable_variables) # calcul du gradient de la focntion loss en fonction des variables du modèle \n",
        "                self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) # optimisation des paramètres du modèle\n",
        "                loss_history[episode] = loss.numpy() # on récupère la valeur pour afficher l'évolution de l'erreur\n",
        "\n",
        "                # update the vector of states\n",
        "                self.update_vec_etat(position)\n",
        "\n",
        "        plt.plot(loss_history, color=\"red\")\n",
        "        plt.plot(reward_history, color=\"blue\")\n",
        "        plt.title(\"Evolution de l'erreur\")\n",
        "        plt.title(\"Evolution du reward\")\n",
        "        plt.show()\n",
        "\n",
        "        # plt.show()\n",
        "\n",
        "        # save the model\n",
        "        self.save_model()\n",
        "\n",
        "    # play the game\n",
        "    def play(self):\n",
        "\n",
        "        iter = 0\n",
        "        position = (0, 0)\n",
        "        fin = False\n",
        "\n",
        "        while iter < self.steps and not fin:\n",
        "            iter += 1\n",
        "            # update the vector of states\n",
        "            self.update_vec_etat(position)\n",
        "            # choose an action\n",
        "            action, _ = self.choose_action(position, 0)\n",
        "            # apply the action\n",
        "            new_position, reward, fin = applicaion_action(action, position, self.space)\n",
        "\n",
        "            print( str(position) + \" \" + str(action) + \" \" + str(new_position))\n",
        "            \n",
        "            if fin:\n",
        "                print(\"fin de partie en\", iter, \"coups\")\n",
        "                break\n",
        "            # update the position\n",
        "            position = new_position\n",
        "\n",
        "        if iter == self.steps:\n",
        "            print(\"Trop d'itérations\")\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Donner quelques éléments de commentaires sur la stratégie que vous avez utilisée pour développer l’apprentissage Deep QL\n",
        "\n",
        "\n",
        "##### 1) La structure du réseau de neurones\n",
        "\n",
        "Nous avons utilisé une structure 3 couches denses ayant 16 entrées (nombre de cases) et 4 sorties (4 actions).\n",
        "\n",
        "##### 2) L'algorithme Deep Q-learning\n",
        "\n",
        "###### Nous nous sommes basés sur le code du cours ***DeepRL*** pour développer notre algorithme Deep Q-learning.\n",
        "\n",
        "*   Nous commençons par créer un ***model_stable*** en clonant notre modèle existant et en fixant ses poids. \n",
        "\n",
        "*   Nous initialisons également un tableau pour enregistrer l'historique des pertes et des récompenses au cours de l'apprentissage.\n",
        "\n",
        "*   Pour chaque épisode, nous réinitialisons la position du joueur et calculons la valeur de l'epsilon.\n",
        "\n",
        "*   Pour chaque étape de l'épisode, nous choisissons une action en utilisant la fonction 'choose_action' en passant en paramètre la position actuelle du joueur et l'epsilon. Nous appliquons ensuite cette action en utilisant la fonction 'applicaion_action' pour obtenir la nouvelle position, la récompense et le booléen indiquant si la partie est fini(dans ce cas,nous sortons de la boucle).\n",
        "\n",
        "*   Tous les 10 étapes, nous mettons à jour les poids du modèle stable avec les poids modèle.\n",
        "\n",
        "*   Nous créons un vecteur ***vec_etat_next*** à la nouvelle position du joueur, qui sera donné en entrée du modèle stable pour prédire la Q_value.\n",
        "\n",
        "*   Nous calculons ***target*** en utilisant la formule du cours, que nous utiliserons avec ***loss_fn*** pour calculer la perte lors de la descente de gradient.\n",
        "\n",
        "*   Nous appliquons la descente de gradient comme vu dans le cours en utilisant ***optimizer*** et ***loss_fn*** données en paramètre.\n",
        "\n",
        "*   Enfin, nous enregistrons la valeur de la perte pour cet épisode dans notre historique et mettons à jour le vecteur d'état en utilisant la nouvelle position du joueur.\n",
        "\n",
        "*   A la fin des épisodes, nous affichons l'évolution de la perte et des récompenses au fil des épisodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "p6GfYriK4m1A"
      },
      "outputs": [],
      "source": [
        "# function to create a game with a configuration train and play\n",
        "def create_game(optimizer_name, loss_fn_name, episodes, steps, dragon_reward, jail_reward, empty_reward, start_reward=-5, verbose=False):\n",
        "    # set the rewards\n",
        "    Rewards.set_rewards({\"S\": start_reward, \"D\": dragon_reward, \"J\": jail_reward, \"_\": empty_reward})\n",
        "    # create the game\n",
        "    game = DeepGame(episodes=episodes, steps=steps, optimizer_name=optimizer_name, loss_fn_name=loss_fn_name, verbose=verbose)\n",
        "\n",
        "    game.train()\n",
        "    game.play()\n",
        "\n",
        "    return game"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w0TgfG6B6UVS"
      },
      "source": [
        "#### TEST\n",
        "\n",
        "*   ***optimizer*** : Adam\n",
        "*   ***loss function*** : MAE\n",
        "*   ***epochs*** : 1000\n",
        "*   ***steps*** : 100\n",
        "*   ***Reward Dragon*** : -500\n",
        "*   ***Reward Empty*** : -20\n",
        "*   ***Reward Jail*** : 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C9ocj0r97kU8",
        "outputId": "3feb6d94-4353-467e-d91d-b313068c01b9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "(0, 0) DROITE (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "(0, 1) HAUT (0, 1)\n",
            "Trop d'itérations\n"
          ]
        }
      ],
      "source": [
        "#game1 = create_game(optimizer_name=\"Adam\", loss_fn_name=\"MAE\", episodes=100, steps=100, dragon_reward=-10, empty_reward=0, jail_reward=10, verbose=False)\n",
        "game1 = create_game(optimizer_name=\"Nadam\", loss_fn_name=\"MSE\", episodes=10, steps=100, dragon_reward=-100, empty_reward=0, jail_reward=100, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5uSY-7_7agv"
      },
      "source": [
        "#### TEST\n",
        "\n",
        "\n",
        "*   ***optimizer*** : Nadam\n",
        "*   ***loss function*** : MSE\n",
        "*   ***epochs*** : 5000\n",
        "*   ***steps*** : 100\n",
        "*   ***Reward Dragon*** : -100\n",
        "*   ***Reward Empty*** : -10\n",
        "*   ***Reward Jail*** : 1000\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_LPDJcA7c11",
        "outputId": "f7d84157-9ff3-4836-d901-10860a2812a3"
      },
      "outputs": [],
      "source": [
        "#game2 = create_game(optimizer_name=\"Nadam\", loss_fn_name=\"MSE\", episodes=3000, steps=100, dragon_reward=-100, empty_reward=-10, jail_reward=1000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4428cbe1ba9314b3551257500664b995dcc328d303584ff4cad6f1a703111ed9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
