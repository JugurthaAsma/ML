{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class for the space\n",
    "class Space:\n",
    "    \"\"\"\n",
    "     constructor by default the default space is used\n",
    "     the space is a 2D array 4 * 4 of characters\n",
    "        the characters are: \n",
    "            'S' : the starting point\n",
    "            '_' : empty space\n",
    "            'J' : the goal\n",
    "            'D' : a dragon\n",
    "    the default space is:\n",
    "        S___\n",
    "        D_D_ \n",
    "        ___D\n",
    "        _D_J\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.space = [\n",
    "            ['S', '_', '_', '_'],\n",
    "            ['D', '_', 'D', '_'],\n",
    "            ['_', '_', '_', 'D'],\n",
    "            ['_', 'D', '_', 'J']\n",
    "        ]\n",
    "\n",
    "    # a constructor that a number of lines and columns and a number of dragons\n",
    "    def __init__(self, lines, columns, dragons):\n",
    "        self.space = []\n",
    "        for l in range(lines):\n",
    "            self.space.append([])\n",
    "            for c in range(columns):\n",
    "                self.space[l].append('_')\n",
    "\n",
    "        self.space[0][0] = 'S'\n",
    "        self.space[lines-1][columns-1] = 'J'\n",
    "        \n",
    "        i = 0\n",
    "        while i < dragons:\n",
    "            l = random.randint(0, lines-1)\n",
    "            c = random.randint(0, columns-1)\n",
    "            if self.space[l][c] == '_':\n",
    "                self.space[l][c] = 'D'\n",
    "                i += 1\n",
    "            else:\n",
    "                i -= 1\n",
    "\n",
    "    # a method the pretty print the space\n",
    "    def print_space(self):\n",
    "        for l in self.space:\n",
    "            for c in l:\n",
    "                print(c, end='| ')\n",
    "            \n",
    "            print()\n",
    "\n",
    "    # a method to get the size of the lines\n",
    "    def get_lines_size(self):\n",
    "        return len(self.space)\n",
    "\n",
    "    # a method to get the size of the columns\n",
    "    def get_columns_size(self):\n",
    "        return len(self.space[0])\n",
    "\n",
    "    # a method to get the size of the space\n",
    "    def get_size(self):\n",
    "        return self.get_lines_size() * self.get_columns_size()\n",
    "    \n",
    "# a static class for the rewards\n",
    "class Rewards:\n",
    "    # the rewards for each character\n",
    "    rewards = {\n",
    "        'S': 0,\n",
    "        '_': 0,\n",
    "        'J': 100,\n",
    "        'D': -100\n",
    "    }\n",
    "\n",
    "    # a method to get the reward of a character\n",
    "    def get_reward(character):\n",
    "        return Rewards.rewards.get(character)\n",
    "    \n",
    "# a static class for the Directions\n",
    "class Directions:\n",
    "    # the directions\n",
    "    directions = [\"HAUT\", \"DROITE\", \"BAS\", \"GAUCHE\"]\n",
    "\n",
    "    # a method to get the size of the directions\n",
    "    def get_size():\n",
    "        return len(Directions.directions)\n",
    "\n",
    "    # a method to get the index of a direction\n",
    "    def get_index(direction):\n",
    "        return Directions.directions.index(direction)\n",
    "\n",
    "    # a method to get a random direction\n",
    "    def get_random_direction():\n",
    "        return random.choice(Directions.directions)\n",
    "\n",
    "    # a method to get the direction that maximizes the Q value\n",
    "    def get_max_direction(mat_q, state):\n",
    "        return Directions.directions[np.argmax(mat_q[state])]\n",
    "\n",
    "# a class for the player\n",
    "class Player:\n",
    "    # constructor by default the player is at the starting point\n",
    "    def __init__(self):\n",
    "        self.position = (0, 0)\n",
    "\n",
    "    # a constructor that takes a position\n",
    "    def __init__(self, position):\n",
    "        self.position = position\n",
    "\n",
    "\n",
    "# a class for the game\n",
    "class Game:\n",
    "    # constructor that takes :\n",
    "    # GAMMA : 0.96 by default\n",
    "    # ALPHA : 0.81 by default\n",
    "    # number of episodes : 10000 by default\n",
    "    # number of steps : 100 by default\n",
    "    # is_random_space : False by default\n",
    "    # a Q matrix : initialized with zeros (with the size of the space and the number of directions)\n",
    "\n",
    "    def __init__(self, GAMMA = 0.96, ALPHA = 0.81, episodes = 10000, steps = 100, is_random_space = False):\n",
    "        self.GAMMA = GAMMA\n",
    "        self.ALPHA = ALPHA\n",
    "        self.episodes = episodes\n",
    "        self.steps = steps\n",
    "        self.player = Player()\n",
    "\n",
    "        # the space\n",
    "        if is_random_space:\n",
    "            self.space = Space(4, 4, 3)\n",
    "        else:\n",
    "            self.space = Space()\n",
    "\n",
    "    \n",
    "        # the Q matrix \n",
    "        self.mat_q = np.zeros((self.space.get_lines_size * self.space.get_columns_size(), Directions.get_size()))\n",
    "\n",
    "    # a method to update the Q matrix\n",
    "    def update_q(self, action, reward, next_state):\n",
    "        state = self.player.position \n",
    "        self.mat_q[state][Directions.get_index(action)] += self.ALPHA * (reward + self.GAMMA * np.max(self.mat_q[next_state]) - self.mat_q[state][Directions.get_index(action)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS = {\" \":0.0, \"D\":-1.0, \"J\":12.0, \"S\":0.0}\n",
    "DIRECTIONS = [\n",
    "    \"HAUT\",\n",
    "    \"DROITE\",\n",
    "    \"BAS\",\n",
    "    \"GAUCHE\",\n",
    "]\n",
    "\n",
    "Nparties = 10000\n",
    "Ncoups = 100\n",
    "alpha = 0.81\n",
    "gamma = 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_space(lines, columns, nbDragon):\n",
    "    space = []\n",
    "    for l in range(lines):\n",
    "        space.append([])\n",
    "        for c in range(columns):\n",
    "            space[l].append(' ')\n",
    "    space[0][0] = 'S'\n",
    "    space[lines-1][columns-1] = 'J'\n",
    "\n",
    "    i = 0\n",
    "    while i < nbDragon: \n",
    "        l = random.randint(0, lines-1)\n",
    "        c = random.randint(0, columns-1)\n",
    "        if space[l][c] == ' ':\n",
    "            space[l][c] = 'D'\n",
    "            i += 1\n",
    "        else:\n",
    "            i -= 1\n",
    "    return space\n",
    "\n",
    "def init_space():\n",
    "    space = init_random_space(4, 4, 3)\n",
    "    return space;\n",
    "\n",
    "def isWin (space, position):\n",
    "    (l,c) = position\n",
    "\n",
    "    if(space[l][c]== 'J'):\n",
    "        return True\n",
    "\n",
    "\n",
    "# a method to apply an action to the player\n",
    "# returns [position, reward, fin]\n",
    "def applicaion_action(action, position, space):\n",
    "    (l, c) = position\n",
    "    nextPos = position\n",
    "\n",
    "    if action == \"HAUT\":\n",
    "        nextPos = (l-1,c)\n",
    "    elif action == \"DROITE\":\n",
    "        nextPos = (l,c+1)\n",
    "    elif action == \"BAS\":\n",
    "        nextPos = (l+1,c);\n",
    "    elif action == \"GAUCHE\":\n",
    "        nextPos = (l,c-1);\n",
    "\n",
    "    # check if the next position is in the space\n",
    "    if (nextPos[0] < len(space) and nextPos[1] < len(space) and nextPos[0] >=0 and nextPos[1] >=0 ):\n",
    "        position = nextPos\n",
    "\n",
    "        # back to the starting point if a dragon is encountered\n",
    "        # get the current case in the space\n",
    "        case = space[position[0]][position[1]]\n",
    "        if case == 'D':\n",
    "            position = (0, 0)\n",
    "    \n",
    "    # set the reward\n",
    "    reward = Rewards.get_reward(case)\n",
    "\n",
    "    # check if the player is at the goal\n",
    "    fin = isWin(space)\n",
    "\n",
    "    return [position, reward, fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = init_space();\n",
    "print(space)\n",
    "#position ddu chevalier\n",
    "player_pos = (0, 0)\n",
    "\n",
    "# for i in range(Nparties):\n",
    "#     print(\"------------------------------- tour \", i)\n",
    "#     # on choisit une direction aléatoire\n",
    "#     action = random.choice(DIRECTIONS)\n",
    "#     print(\"action : \", action)\n",
    "#     # on applique l'action\n",
    "#     player_pos, reward, fin = application_action(action, player_pos, space)\n",
    "#     print(\"position : \", player_pos)\n",
    "#     print(\"reward : \", reward)\n",
    "#     print(\"fin : \", fin)\n",
    "\n",
    "#     # fin de partie\n",
    "#     if fin:\n",
    "#         print(\"fin de partie\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Développement du Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method to choose an action with the epsilon greedy policy\n",
    "def choose_action(state, epsilon, mat_q):\n",
    "    if random.random() < epsilon:\n",
    "        return Directions.get_random_direction()\n",
    "    else:\n",
    "        return Directions.get_max_direction(mat_q, state)\n",
    "\n",
    "# a method to play one step (with mat_q, state, epsilon)\n",
    "def oneStep(mat_q, state, epsilon):\n",
    "    action = choose_action(state, epsilon)\n",
    "    (next_state, reward, fin) = applicaion_action(action, space)\n",
    "    update_q(state, action, reward, next_state)\n",
    "    return [next_state, reward, fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneStep(mat_q, state, epsilon):\n",
    "    # on choisit une action\n",
    "    action = choose_action(state, epsilon, mat_q)\n",
    "    # on applique l'action\n",
    "    new_state, reward, fin = application_action(action, state, space)\n",
    "    # on met à jour la matrice Q\n",
    "    #mat_q[state][DIRECTIONS.index(action)] += alpha * (reward + gamma * (mat_q[new_state][DIRECTIONS.index(choose_action(new_state, epsilon, mat_q))]) - mat_q[state][DIRECTIONS.index(action)])\n",
    "    mat_q[state][DIRECTIONS.index(action)] += alpha * (reward + gamma * np.max(mat_q[new_state]) - mat_q[state][DIRECTIONS.index(action)])\n",
    "    return mat_q, new_state, fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mat_q\n",
    "mat_q = np.zeros((len(space), len(space), len(DIRECTIONS)))\n",
    "\n",
    "totalSteps = 0\n",
    "\n",
    "# on applique l'algorithme\n",
    "for iterationPartie in range(Nparties):\n",
    "    state = (0,0)\n",
    "    # calcul de epsilon\n",
    "    epsilon = Nparties/(Nparties+iterationPartie)\n",
    "    #print(\"epsilon : \", epsilon)\n",
    "\n",
    "    for iterationCoups in range(Ncoups):\n",
    "        mat_q, state, fin = oneStep(mat_q, state, epsilon)\n",
    "        if fin:\n",
    "            #print(\"fin de partie en \", iterationCoups, \" coups, partie \", iterationPartie)\n",
    "            totalSteps += iterationCoups\n",
    "            #print(mat_q)\n",
    "            break\n",
    "\n",
    "print(\"nombre de coups total : \", totalSteps)\n",
    "print(\"moyenne de coups par partie : \", totalSteps/Nparties)\n",
    "\n",
    "#print(mat_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iterationCoups in range(Ncoups):\n",
    "    mat_q, state, fin = oneStep(mat_q, state, 0)\n",
    "    if fin:\n",
    "        print(\"fin de partie en \", iterationCoups, \" coups\")\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sys\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test avec une structure 2 couches denses ayant 16 entrées (nombre de cases) et 4 sorties (4 actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 modifier la fonction choose_action avec en sortir model.predict(vec_etat)\n",
    "def choose_action(vect_etat, epsilon, model):\n",
    "    # L'agent est dans un certain état s, on choisit une action a selon :\n",
    "\n",
    "    # Au hasard avec une probabilité epsilon\n",
    "    if random.random() < epsilon:\n",
    "        # on choisit une action aléatoire\n",
    "        action = random.choice(DIRECTIONS)\n",
    "    else:\n",
    "        # La meilleure avec une probabilité 1-epsilon\n",
    "        Sortie_Q = model.predict(vect_etat, verbose=0)  # En entrée le vecteur symbolisant l'état\n",
    "        action = DIRECTIONS[np.argmax(Sortie_Q)] #On sélectionne l'action associée avec la sortie max\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation du model\n",
    "# Une structure simple avec 16 entrées et 4 sorties, la sortie est sans activation\n",
    "model = Sequential([\n",
    "    Dense(4, activation='relu', input_shape=[16]),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(4),\n",
    "])\n",
    "\n",
    "# En préambule création d’un second modèle\n",
    "model_stable = keras.models.clone_model(model)\n",
    "model_stable.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix de l'optimiseur\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01) \n",
    "\n",
    "# on va définir la fonction de perte\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# on va définir la dérivé de la fonction d'activation\n",
    "\n",
    "# relu\n",
    "@tf.custom_gradient\n",
    "def my_relu(x):\n",
    "    y = tf.nn.relu(x)\n",
    "    def grad(dy):\n",
    "        return dy * tf.cast(x > 0, tf.float32)\n",
    "    return y, grad\n",
    "\n",
    "# tanh\n",
    "@tf.custom_gradient\n",
    "def my_tanh(x):\n",
    "    y = tf.math.tanh(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - y ** 2)\n",
    "    return y, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function d'initialisation\n",
    "def init():\n",
    "    position = (0,0)\n",
    "    # Créer en entrée un vecteur de taille lines * columns avec comme nombre de sorties le nombre d'action possible \n",
    "    vect_etat = np.zeros((1,16))\n",
    "    vect_etat[0, int(len(space) * position[0] + position[1])] = 1\n",
    "\n",
    "    return position, vect_etat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position, vect_etat = init()\n",
    "\n",
    "# ITERATIONS D'APPRENTISSAGE\n",
    "\n",
    "# on applique l'algorithme\n",
    "for iterationPartie in range(Nparties):\n",
    "    progress = \"Partie \" + str(iterationPartie) + \"/\" + str(Nparties) \n",
    "    sys.stdout.write(\"\\r\" + progress)\n",
    "\n",
    "    # on réinitialise la position\n",
    "    position = (0,0)\n",
    "\n",
    "    # calcul de epsilon\n",
    "    epsilon = Nparties/(Nparties+iterationPartie)\n",
    "\n",
    "    fin = False\n",
    "\n",
    "    while not fin:\n",
    "        # on choisit une action\n",
    "        action = choose_action(vect_etat, epsilon, model)\n",
    "\n",
    "        # on applique l'action\n",
    "        new_position, reward, fin = application_action(action, position, space)\n",
    "\n",
    "        # on met à jour vect_etat\n",
    "        vect_etat[0, int(len(space) * position[0] + position[1])] = 0\n",
    "        vect_etat[0, int(len(space) * new_position[0] + new_position[1])] = 1\n",
    "\n",
    "        # model stable\n",
    "        sortie_Q_stable = model_stable.predict(vect_etat, verbose=0)\n",
    "        max_Q = np.max(sortie_Q_stable)\n",
    "\n",
    "        target = reward + gamma * max_Q\n",
    "\n",
    "        # descente de gradient\n",
    "        with tf.GradientTape() as tape:\n",
    "            predict = model(vect_etat, training=True)  #Ce que l'on pense obtenir \n",
    "            mask = tf.one_hot(DIRECTIONS.index(action), len(DIRECTIONS)) #On crée un masque pour sélectionner la sortie correspondant à l'action choisie\n",
    "            val_predict = tf.reduce_sum(predict * mask, axis=1) #On sélectionne la sortie correspondant à l'action choisie\n",
    "            loss = loss_fn(target, val_predict) #On calcule la perte\n",
    "        \n",
    "        # on applique la descente de gradient\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # optimisation des paramètres\n",
    "\n",
    "        # on met à jour la position\n",
    "        position = new_position\n",
    "\n",
    "        # on met à jour le model stable\n",
    "        if iterationPartie % 100 == 0:\n",
    "            model_stable.set_weights(model.get_weights())\n",
    "\n",
    "    \n",
    "# sauvegarde du model\n",
    "model.save('deep_Q_learning_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JEU\n",
    "\n",
    "def play():\n",
    "\n",
    "    # affichage de l'espace avec un retour à la ligne à la fin de chaque ligne\n",
    "    for i in range(len(space)):\n",
    "        for j in range(len(space)):\n",
    "            print(space[i][j], end=\"| \")\n",
    "        print()\n",
    "    # on charge le model\n",
    "    model = keras.models.load_model('deep_Q_learning_model.h5')\n",
    "\n",
    "    # initialisation de la position\n",
    "    position = (0,0)\n",
    "    fin = False\n",
    "\n",
    "    MAX_ITER = 100\n",
    "    iter = 0\n",
    "    while not fin and iter < MAX_ITER:\n",
    "        iter += 1\n",
    "\n",
    "        # on crée le vecteur d'état\n",
    "        vect_etat = np.zeros((1,16))\n",
    "        vect_etat[0, int(len(space) * position[0] + position[1])] = 1\n",
    "\n",
    "        # on choisit une action\n",
    "        action = choose_action(vect_etat, 0, model)\n",
    "\n",
    "        # on applique l'action\n",
    "        new_position, reward, fin = application_action(action, position, space)\n",
    "\n",
    "        # on met à jour la position\n",
    "        position = new_position\n",
    "\n",
    "        # on affiche l'action choisie avec \", \" en end de print pour ne pas faire de retour à la ligne\n",
    "        # mais avec un retour à la ligne tout les 10 actions\n",
    "        print(action, end=\", \")\n",
    "        if iter % 10 == 0:\n",
    "            print()\n",
    "        \n",
    "\n",
    "\n",
    "    if fin:\n",
    "        print(\"Victoire ! en \" + str(iter) + \" itérations\")\n",
    "    else:\n",
    "        print(\"Défaite\")\n",
    "        if (iter >= MAX_ITER):\n",
    "            print(\"Trop d'itérations\")\n",
    "\n",
    "play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4428cbe1ba9314b3551257500664b995dcc328d303584ff4cad6f1a703111ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
